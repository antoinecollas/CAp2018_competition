{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Préparation des données avec ELMO à la place de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/train_cap2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size = 0.2\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21848, 60)\n",
      "(5462, 60)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataset, test_size=test_size, random_state=random_state, shuffle=True, stratify=dataset.loc[:,'level1'])\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21848, 59)\n",
      "(21848,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train.iloc[:, :-1]\n",
    "y_train = train.iloc[:, -1]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace({\"A1\": 0, \"A2\" : 1, \"B1\" : 2, \"B2\" : 3, \"C1\" : 4, \"C2\" : 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_fulltext = X_train.loc[:, 'fulltext'].iloc[:10]\n",
    "#y_train = y_train[:10]\n",
    "#print(X_train_fulltext)\n",
    "\n",
    "X_train_fulltext = X_train.loc[:, 'fulltext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 0.7\n",
    "\n",
    "X_train = X_train_fulltext[0:int(X_train_fulltext.shape[0]*training_size)]\n",
    "y_train_full = y_train\n",
    "y_train = y_train_full[0:int(X_train_fulltext.shape[0]*training_size)]\n",
    "\n",
    "X_val = X_train_fulltext[int(X_train_fulltext.shape[0]*training_size):]\n",
    "y_val = y_train_full[int(X_train_fulltext.shape[0]*training_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train.reset_index().iloc[:,1])\n",
    "y_val = np.array(y_val.reset_index().iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6555,)\n",
      "(6555,)\n",
      "(15293,)\n",
      "(15293,)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmation avec mode \"eager execution\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération d'un module ELMO déjà entrainé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(X, iteration, batch_size):\n",
    "    X_batch = X[iteration*batch_size:(iteration+1)*batch_size]\n",
    "    return X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_RNN_and_train(X_train, y_train, X_val, y_val, n_steps, n_neurons=500, activation=tf.nn.relu, \n",
    "                               dropout_in=0, dropout_out=0, class_weights=[1, 1, 1, 1, 1, 1], learning_rate=0.001, \n",
    "                               n_epochs=100, batch_size=200, max_checks_without_progress=3):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    X = tf.placeholder(dtype=tf.string, shape=[None], name=\"X\")\n",
    "    y = tf.placeholder(tf.int64, shape=[None], name=\"y\")\n",
    "    \n",
    "    elmo = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=True)\n",
    "    X_elmo = elmo(X, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    #X_elmo = tf.cast(X_elmo, tf.float16)\n",
    "    sequence_length = tf.shape(X_elmo)[1]*tf.ones(tf.shape(X_elmo)[0], dtype=tf.int32)\n",
    "    \n",
    "    dropout_in_placeholder = tf.placeholder_with_default(tf.constant(0.0, dtype=tf.float32), ())\n",
    "    dropout_out_placeholder = tf.placeholder_with_default(tf.constant(0.0, dtype=tf.float32), ())\n",
    "    \n",
    "    basic_cell = tf.contrib.rnn.GRUCell(num_units=n_neurons, activation=activation)\n",
    "    basic_cell = tf.contrib.rnn.DropoutWrapper(basic_cell, input_keep_prob=1-dropout_in_placeholder, output_keep_prob=1-dropout_out_placeholder, dtype=tf.float32)\n",
    "    outputs, states = tf.nn.dynamic_rnn(basic_cell, X_elmo, sequence_length=sequence_length, dtype=tf.float32)\n",
    "\n",
    "    logits = tf.layers.dense(inputs=states, units=n_outputs, name=\"logits\")\n",
    "    inference = tf.nn.softmax(logits, name=\"inference\")\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        #loss = cost(inference, y)\n",
    "\n",
    "        class_weights_tf = tf.constant(class_weights)\n",
    "        weights = tf.gather(class_weights_tf, y)\n",
    "        xentropy = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits, weights=weights)\n",
    "        #xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y) #ancienne version (sans poids)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(tf.cast(logits, tf.float32), y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(\"./summary\", tf.get_default_graph())\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    n_batches_per_epoch = X_train.shape[0] // batch_size\n",
    "    print(\"Nombre de batchs par epoch =\", n_batches_per_epoch)\n",
    "    \n",
    "    best_loss = np.infty\n",
    "    checks_without_progress = 0\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                if (iteration+1)%10==0:\n",
    "                    print(\"Batch n°\", iteration+1)\n",
    "                X_batch = get_next_batch(X_train, iteration, batch_size)\n",
    "                #print(\"X_batch=\", X_batch)\n",
    "                y_batch = y_train[iteration*batch_size:(iteration+1)*batch_size]\n",
    "                #print(\"y_batch=\", y_batch)\n",
    "\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch, dropout_in_placeholder: dropout_in, dropout_out_placeholder: dropout_out})\n",
    "                #print(sess.run(X_elmo, feed_dict={X: X_batch, y: y_batch, dropout_in_placeholder: dropout_in, dropout_out_placeholder: dropout_out}))\n",
    "\n",
    "            #fonction de coût sur les 5000 premiers textes d'entrainement (pour que ça tienne dans la mémoire vive)\n",
    "            nb_training_examples = 30\n",
    "            if X_train.shape[0] < nb_training_examples:\n",
    "                nb_training_examples = X_train.shape[0]\n",
    "            loss_train = loss.eval(feed_dict={X: get_next_batch(X_train[0:nb_training_examples], 0, nb_training_examples), y: y_train[0:nb_training_examples]})\n",
    "            loss_val = loss.eval(feed_dict={X: get_next_batch(X_val[0:nb_training_examples], 0, nb_training_examples), y: y_val[0:nb_training_examples]})\n",
    "            print(epoch, \"Loss training:\", loss_train)\n",
    "            print(epoch, \"Loss validation:\", loss_val)\n",
    "\n",
    "            if loss_val < best_loss:\n",
    "                save_path = saver.save(sess, \"./natural_language_classifier.ckpt\")\n",
    "                best_loss = loss_val\n",
    "                checks_without_progress = 0\n",
    "            else:\n",
    "                checks_without_progress += 1\n",
    "                if checks_without_progress >= MAX_CHECKS_WITHOUT_PROGRESS:\n",
    "                    print(\"Early stopping!\")\n",
    "                    break\n",
    "    return inference, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre maximal de mots par texte (fixe) = 450\n"
     ]
    }
   ],
   "source": [
    "n_steps = 450 #taille maximale des textes (rendue fixe)\n",
    "print(\"Nombre maximal de mots par texte (fixe) =\", n_steps)\n",
    "n_neurons = 500\n",
    "activation = tf.nn.relu\n",
    "n_outputs = 6\n",
    "class_weights = [1, 1, 1, 1, 1, 1] #poids de la fonction de coût\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 30\n",
    "\n",
    "MAX_CHECKS_WITHOUT_PROGRESS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/aggregation/scaling:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with aggregation/scaling\n",
      "INFO:tensorflow:Initialize variable module/aggregation/weights:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with aggregation/weights\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_0:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/W_cnn_0\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_1:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/W_cnn_1\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_2:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/W_cnn_2\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_3:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/W_cnn_3\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_4:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/W_cnn_4\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_5:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/W_cnn_5\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_6:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/W_cnn_6\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_0:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/b_cnn_0\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_1:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/b_cnn_1\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_2:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/b_cnn_2\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_3:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/b_cnn_3\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_4:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/b_cnn_4\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_5:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/b_cnn_5\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_6:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN/b_cnn_6\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_0/W_carry:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN_high_0/W_carry\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_0/W_transform:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN_high_0/W_transform\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_0/b_carry:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN_high_0/b_carry\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_0/b_transform:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN_high_0/b_transform\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_1/W_carry:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN_high_1/W_carry\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_1/W_transform:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN_high_1/W_transform\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_1/b_carry:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN_high_1/b_carry\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_1/b_transform:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN_high_1/b_transform\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_proj/W_proj:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN_proj/W_proj\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_proj/b_proj:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/CNN_proj/b_proj\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/char_embed:0 from checkpoint b'/tmp/tfhub_modules/0eadd30f548cbfd3065119697ded6bb381668285/variables/variables' with bilm/char_embed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de batchs par epoch = 509\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "Batch n° 120\n",
      "Batch n° 130\n",
      "Batch n° 140\n",
      "Batch n° 150\n",
      "Batch n° 160\n",
      "Batch n° 170\n",
      "Batch n° 180\n",
      "Batch n° 190\n",
      "Batch n° 200\n",
      "Batch n° 210\n",
      "Batch n° 220\n",
      "Batch n° 230\n",
      "Batch n° 240\n",
      "Batch n° 250\n",
      "Batch n° 260\n",
      "Batch n° 270\n",
      "Batch n° 280\n",
      "Batch n° 290\n",
      "Batch n° 300\n",
      "Batch n° 310\n",
      "Batch n° 320\n",
      "Batch n° 330\n",
      "Batch n° 340\n",
      "Batch n° 350\n",
      "Batch n° 360\n",
      "Batch n° 370\n",
      "Batch n° 380\n",
      "Batch n° 390\n",
      "Batch n° 400\n",
      "Batch n° 410\n",
      "Batch n° 420\n",
      "Batch n° 430\n",
      "Batch n° 440\n",
      "Batch n° 450\n",
      "Batch n° 460\n",
      "Batch n° 470\n",
      "Batch n° 480\n",
      "Batch n° 490\n",
      "Batch n° 500\n",
      "0 Loss training: 0.6042422\n",
      "0 Loss validation: 0.81565005\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "Batch n° 120\n",
      "Batch n° 130\n",
      "Batch n° 140\n",
      "Batch n° 150\n",
      "Batch n° 160\n",
      "Batch n° 170\n",
      "Batch n° 180\n",
      "Batch n° 190\n",
      "Batch n° 200\n",
      "Batch n° 210\n",
      "Batch n° 220\n",
      "Batch n° 230\n",
      "Batch n° 240\n",
      "Batch n° 250\n",
      "Batch n° 260\n",
      "Batch n° 270\n",
      "Batch n° 280\n",
      "Batch n° 290\n",
      "Batch n° 300\n",
      "Batch n° 310\n",
      "Batch n° 320\n",
      "Batch n° 330\n",
      "Batch n° 340\n",
      "Batch n° 350\n",
      "Batch n° 360\n",
      "Batch n° 370\n",
      "Batch n° 380\n",
      "Batch n° 390\n",
      "Batch n° 400\n",
      "Batch n° 410\n",
      "Batch n° 420\n",
      "Batch n° 430\n",
      "Batch n° 440\n",
      "Batch n° 450\n",
      "Batch n° 460\n",
      "Batch n° 470\n",
      "Batch n° 480\n",
      "Batch n° 490\n",
      "Batch n° 500\n",
      "1 Loss training: 0.5324419\n",
      "1 Loss validation: 0.9264144\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "Batch n° 120\n",
      "Batch n° 130\n",
      "Batch n° 140\n",
      "Batch n° 150\n",
      "Batch n° 160\n",
      "Batch n° 170\n",
      "Batch n° 180\n",
      "Batch n° 190\n",
      "Batch n° 200\n",
      "Batch n° 210\n",
      "Batch n° 220\n",
      "Batch n° 230\n",
      "Batch n° 240\n",
      "Batch n° 250\n",
      "Batch n° 260\n",
      "Batch n° 270\n",
      "Batch n° 280\n",
      "Batch n° 290\n",
      "Batch n° 300\n",
      "Batch n° 310\n",
      "Batch n° 320\n",
      "Batch n° 330\n",
      "Batch n° 340\n",
      "Batch n° 350\n",
      "Batch n° 360\n",
      "Batch n° 370\n",
      "Batch n° 380\n",
      "Batch n° 390\n",
      "Batch n° 400\n",
      "Batch n° 410\n",
      "Batch n° 420\n",
      "Batch n° 430\n",
      "Batch n° 440\n",
      "Batch n° 450\n",
      "Batch n° 460\n",
      "Batch n° 470\n",
      "Batch n° 480\n",
      "Batch n° 490\n"
     ]
    }
   ],
   "source": [
    "inference, X = create_graph_RNN_and_train(X_train, y_train, X_val, y_val, n_steps, n_neurons=n_neurons, \n",
    "                                          activation=activation, class_weights=class_weights,\n",
    "                                           learning_rate=learning_rate, n_epochs=n_epochs, \n",
    "                                          batch_size=batch_size, max_checks_without_progress=MAX_CHECKS_WITHOUT_PROGRESS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A modifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = np.array([[0,1,2,3,4,6],[1,0,1,4,5,8],[3,2,0,3,5,8],[10,7,5,0,2,7],[20,16,12,4,0,8],[44,38,32,19,13,0]])\n",
    "names = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes=['A1', 'A2', 'B1', 'B2', 'C1', 'C2'],\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print_confusion = True\n",
    "def cost(y_pred, y_true, normalize=True):\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    res = (1/y_true.shape[0]) * np.sum(np.multiply(costs, confusion))\n",
    "    \n",
    "    if print_confusion:\n",
    "        # Compute confusion matrix\n",
    "        cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        np.set_printoptions(precision=2)\n",
    "\n",
    "        # Plot normalized confusion matrix\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cnf_matrix, normalize=normalize, title='Normalized confusion matrix')\n",
    "\n",
    "        plt.show()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion = True\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./natural_language_classifier.ckpt\")\n",
    "    res = sess.run(inference, feed_dict={X: get_next_batch(X_val, 0, X_val.shape[0])})\n",
    "    y_pred = np.argmax(res, axis=1)\n",
    "\n",
    "print(cost(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
