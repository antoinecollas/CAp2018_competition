{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPw_A2ta3vim"
   },
   "source": [
    "# Chargement des données déjà préparées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les news sont les 10000 derniers fichiers de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vZTDQYA73vim"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "s_g3WVMx3viq"
   },
   "outputs": [],
   "source": [
    "def load(list_files):\n",
    "    list_files = np.sort(list_files)\n",
    "    res = []\n",
    "    for i in range(list_files.shape[0]):\n",
    "        if i%100==0 and i > 0:\n",
    "            print(\"i=\", i)\n",
    "        res.append(sparse.load_npz(file=list_files[i]))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "directory = \"texts_matrices_with_news\"\n",
    "directory2 = \"size_texts_with_news\"\n",
    "directory3 = \"y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des données d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_NEWS = 7999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 100\n",
      "i= 200\n",
      "i= 300\n",
      "i= 400\n",
      "i= 500\n",
      "i= 600\n",
      "i= 700\n",
      "i= 800\n",
      "i= 900\n",
      "i= 1000\n",
      "i= 1100\n",
      "i= 1200\n",
      "i= 1300\n",
      "i= 1400\n",
      "i= 1500\n",
      "i= 1600\n",
      "i= 1700\n",
      "i= 1800\n",
      "i= 1900\n",
      "i= 2000\n",
      "i= 2100\n",
      "i= 2200\n",
      "i= 2300\n",
      "i= 2400\n",
      "i= 2500\n",
      "i= 2600\n",
      "i= 2700\n",
      "i= 2800\n",
      "i= 2900\n",
      "i= 3000\n",
      "i= 3100\n",
      "i= 3200\n",
      "i= 3300\n",
      "i= 3400\n",
      "i= 3500\n",
      "i= 3600\n",
      "i= 3700\n",
      "i= 3800\n",
      "i= 3900\n",
      "i= 4000\n",
      "i= 4100\n",
      "i= 4200\n",
      "i= 4300\n",
      "i= 4400\n",
      "i= 4500\n",
      "i= 4600\n",
      "i= 4700\n",
      "i= 4800\n",
      "i= 4900\n",
      "i= 5000\n",
      "i= 5100\n",
      "i= 5200\n",
      "i= 5300\n",
      "i= 5400\n",
      "i= 5500\n",
      "i= 5600\n",
      "i= 5700\n",
      "i= 5800\n",
      "i= 5900\n",
      "i= 6000\n",
      "i= 6100\n",
      "i= 6200\n",
      "i= 6300\n",
      "i= 6400\n",
      "i= 6500\n",
      "i= 6600\n",
      "i= 6700\n",
      "i= 6800\n",
      "i= 6900\n",
      "i= 7000\n",
      "i= 7100\n",
      "i= 7200\n",
      "i= 7300\n",
      "i= 7400\n",
      "i= 7500\n",
      "i= 7600\n",
      "i= 7700\n",
      "i= 7800\n",
      "i= 7900\n",
      "i= 8000\n",
      "i= 8100\n",
      "i= 8200\n",
      "i= 8300\n",
      "i= 8400\n",
      "i= 8500\n",
      "i= 8600\n",
      "i= 8700\n",
      "i= 8800\n",
      "i= 8900\n",
      "i= 9000\n",
      "i= 9100\n",
      "i= 9200\n",
      "i= 9300\n",
      "i= 9400\n",
      "i= 9500\n",
      "i= 9600\n",
      "i= 9700\n",
      "i= 9800\n",
      "i= 9900\n",
      "i= 10000\n",
      "i= 10100\n",
      "i= 10200\n",
      "i= 10300\n",
      "i= 10400\n",
      "i= 10500\n",
      "i= 10600\n",
      "i= 10700\n",
      "i= 10800\n",
      "i= 10900\n",
      "i= 11000\n",
      "i= 11100\n",
      "i= 11200\n",
      "i= 11300\n",
      "i= 11400\n",
      "i= 11500\n",
      "i= 11600\n",
      "i= 11700\n",
      "i= 11800\n",
      "i= 11900\n",
      "i= 12000\n",
      "i= 12100\n",
      "i= 12200\n",
      "i= 12300\n",
      "i= 12400\n",
      "i= 12500\n",
      "i= 12600\n",
      "i= 12700\n",
      "i= 12800\n",
      "i= 12900\n",
      "i= 13000\n",
      "i= 13100\n",
      "i= 13200\n",
      "i= 13300\n",
      "i= 13400\n",
      "i= 13500\n",
      "i= 13600\n",
      "i= 13700\n",
      "i= 13800\n",
      "i= 13900\n",
      "i= 14000\n",
      "i= 14100\n",
      "i= 14200\n",
      "i= 14300\n",
      "i= 14400\n",
      "i= 14500\n",
      "i= 14600\n",
      "i= 14700\n",
      "i= 14800\n",
      "i= 14900\n",
      "i= 15000\n",
      "i= 15100\n",
      "i= 15200\n",
      "i= 15300\n",
      "i= 15400\n",
      "i= 15500\n",
      "i= 15600\n",
      "i= 15700\n",
      "i= 15800\n",
      "i= 15900\n",
      "i= 16000\n",
      "i= 16100\n",
      "i= 16200\n",
      "i= 16300\n",
      "i= 16400\n",
      "i= 16500\n",
      "i= 16600\n",
      "i= 16700\n",
      "i= 16800\n",
      "i= 16900\n",
      "i= 17000\n",
      "i= 17100\n",
      "i= 17200\n",
      "i= 17300\n",
      "i= 17400\n",
      "i= 17500\n",
      "i= 17600\n",
      "i= 17700\n",
      "i= 17800\n",
      "i= 17900\n",
      "i= 18000\n",
      "i= 18100\n",
      "i= 18200\n",
      "i= 18300\n",
      "i= 18400\n",
      "i= 18500\n",
      "i= 18600\n",
      "i= 18700\n",
      "i= 18800\n",
      "i= 18900\n",
      "i= 19000\n",
      "i= 19100\n",
      "i= 19200\n",
      "i= 19300\n",
      "i= 19400\n",
      "i= 19500\n",
      "i= 19600\n",
      "i= 19700\n",
      "i= 19800\n",
      "i= 19900\n",
      "i= 20000\n",
      "i= 20100\n",
      "i= 20200\n",
      "i= 20300\n",
      "i= 20400\n",
      "i= 20500\n",
      "i= 20600\n",
      "i= 20700\n",
      "i= 20800\n",
      "i= 20900\n",
      "i= 21000\n",
      "i= 21100\n",
      "i= 21200\n",
      "i= 21300\n",
      "i= 21400\n",
      "i= 21500\n",
      "i= 21600\n",
      "i= 21700\n",
      "i= 21800\n",
      "i= 21900\n",
      "i= 22000\n",
      "i= 22100\n",
      "i= 22200\n",
      "i= 22300\n",
      "i= 22400\n",
      "i= 22500\n",
      "i= 22600\n",
      "i= 22700\n",
      "i= 22800\n",
      "i= 22900\n",
      "i= 23000\n",
      "i= 23100\n",
      "i= 23200\n",
      "(23292,)\n",
      "(23292,)\n",
      "(23292,)\n"
     ]
    }
   ],
   "source": [
    "list_files = np.array(glob.glob(\"./\" + directory + \"/train/*.npz\"))\n",
    "X_train = load(list_files)\n",
    "X_train = X_train[:-(8000-NB_NEWS)]\n",
    "print(X_train.shape)\n",
    "\n",
    "#test_dense_load = sparse.csr_matrix.todense(X_train[0])\n",
    "#print(test_dense_load)\n",
    "#print(test_dense_load.shape)\n",
    "\n",
    "filename = \"./\" + directory2 +\"/train.npy\"\n",
    "size_texts_train = np.load(filename)\n",
    "size_texts_train = size_texts_train[:-(8000-NB_NEWS)]\n",
    "print(size_texts_train.shape)\n",
    "\n",
    "filename = \"./\" + directory3 +\"/train.npy\"\n",
    "y_train = np.load(filename)\n",
    "y_train = y_train[:-(8000-NB_NEWS)]\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des données d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 100\n",
      "i= 200\n",
      "i= 300\n",
      "i= 400\n",
      "i= 500\n",
      "i= 600\n",
      "i= 700\n",
      "i= 800\n",
      "i= 900\n",
      "i= 1000\n",
      "i= 1100\n",
      "i= 1200\n",
      "i= 1300\n",
      "i= 1400\n",
      "i= 1500\n",
      "i= 1600\n",
      "i= 1700\n",
      "i= 1800\n",
      "i= 1900\n",
      "i= 2000\n",
      "i= 2100\n",
      "i= 2200\n",
      "i= 2300\n",
      "i= 2400\n",
      "i= 2500\n",
      "i= 2600\n",
      "i= 2700\n",
      "i= 2800\n",
      "i= 2900\n",
      "i= 3000\n",
      "i= 3100\n",
      "i= 3200\n",
      "i= 3300\n",
      "i= 3400\n",
      "i= 3500\n",
      "i= 3600\n",
      "i= 3700\n",
      "i= 3800\n",
      "i= 3900\n",
      "i= 4000\n",
      "i= 4100\n",
      "i= 4200\n",
      "i= 4300\n",
      "i= 4400\n",
      "i= 4500\n",
      "i= 4600\n",
      "i= 4700\n",
      "i= 4800\n",
      "i= 4900\n",
      "i= 5000\n",
      "i= 5100\n",
      "i= 5200\n",
      "i= 5300\n",
      "i= 5400\n",
      "i= 5500\n",
      "i= 5600\n",
      "i= 5700\n",
      "i= 5800\n",
      "i= 5900\n",
      "i= 6000\n",
      "i= 6100\n",
      "i= 6200\n",
      "i= 6300\n",
      "i= 6400\n",
      "i= 6500\n",
      "(6555,)\n",
      "(6555,)\n",
      "(6555,)\n"
     ]
    }
   ],
   "source": [
    "list_files = np.array(glob.glob(\"./\" + directory + \"/val/*.npz\"))\n",
    "X_val = load(list_files)\n",
    "print(X_val.shape)\n",
    "\n",
    "filename = \"./\" + directory2 +\"/val.npy\"\n",
    "size_texts_val = np.load(filename)\n",
    "print(size_texts_val.shape)\n",
    "\n",
    "filename = \"./\" + directory3 +\"/val.npy\"\n",
    "y_val = np.load(filename)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Répartition des niveaux dans l'ensemble d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  6379\n",
      "1  :  4332\n",
      "2  :  2968\n",
      "3  :  1319\n",
      "4  :  269\n",
      "5  :  8025\n"
     ]
    }
   ],
   "source": [
    "for i in np.unique(y_train):\n",
    "    print(i, ' : ', (y_train == i).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Répartition des niveaux dans l'ensemble d'évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  2710\n",
      "1  :  1818\n",
      "2  :  1338\n",
      "3  :  551\n",
      "4  :  124\n",
      "5  :  14\n"
     ]
    }
   ],
   "source": [
    "for i in np.unique(y_val):\n",
    "    print(i, ' : ', (y_val == i).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les 10000 derniers textes de X_train sont les news. On mélange pour avoir une meilleur répartition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23292,)\n",
      "[    0     1     2 ... 23289 23290 23291]\n",
      "[ 1232 23288  8276 ...  1996   317 17576]\n",
      "(23292,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "s = np.arange(X_train.shape[0])\n",
    "print(s)\n",
    "\n",
    "np.random.shuffle(s)\n",
    "print(s)\n",
    "X_train = X_train[s]\n",
    "y_train = y_train[s]\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch_X(X, X_lengths, iteration, batch_size):\n",
    "    X_batch = X[iteration*batch_size:(iteration+1)*batch_size]\n",
    "    X_batch_lengths = X_lengths[iteration*batch_size:(iteration+1)*batch_size]\n",
    "    res = np.zeros((X_batch.shape[0], X_batch[0].shape[0], X_batch[0].shape[1]))\n",
    "    for i in range(len(X_batch)):\n",
    "        #print(X_batch[i])\n",
    "        #print(type(X_batch[i]))\n",
    "        res[i,:,:] = sparse.csr_matrix.todense(X_batch[i])\n",
    "    #X_batch = np.reshape(X_batch, (batch_size, X_batch[0].shape[0], X_batch[0].shape[1]))\n",
    "    return res, X_batch_lengths\n",
    "\n",
    "def get_next_batch_y(y, iteration, batch_size):\n",
    "    y_batch = y[iteration*batch_size:(iteration+1)*batch_size]\n",
    "    return y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_RNN_and_train(X_train, y_train, X_train_lengths, X_val, y_val, X_val_lengths, n_steps, n_inputs, \n",
    "                               n_neurons=500, activation=tf.nn.tanh, \n",
    "                               dropout_in=0, dropout_out=0, class_weights=[1, 1, 1, 1, 1, 1], learning_rate=0.001, \n",
    "                               n_epochs=100, batch_size=200, max_checks_without_progress=3):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs], name=\"X\")\n",
    "    seq_lengths = tf.placeholder(tf.int32, [None]) #vecteur avec les nombres de mots dans les textes\n",
    "    y = tf.placeholder(tf.int64, shape=[None], name=\"y\")\n",
    "    \n",
    "    dropout_in_placeholder = tf.placeholder_with_default(tf.constant(0.0, dtype=tf.float32), ())\n",
    "    dropout_out_placeholder = tf.placeholder_with_default(tf.constant(0.0, dtype=tf.float32), ())\n",
    "    \n",
    "    basic_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, activation=activation)\n",
    "    basic_cell = tf.contrib.rnn.DropoutWrapper(basic_cell, input_keep_prob=1-dropout_in_placeholder, output_keep_prob=1-dropout_out_placeholder, dtype=tf.float32)\n",
    "    outputs, states = tf.nn.dynamic_rnn(basic_cell, X, sequence_length=seq_lengths, dtype=tf.float32)\n",
    "    \n",
    "    #real_outputs = outputs[:,:,seq_lengths] #1 à retirer\n",
    "    idx = tf.range(tf.shape(X)[0])*tf.shape(outputs)[1] + (seq_lengths - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(outputs, [-1, n_neurons]), idx)\n",
    "    #taille de l'output: [batch_size, n_steps, n_neurons] (par exemple: (200, 418, 500))\n",
    "    #comme les textes sont de tailles variables, on ne peut pas utiliser l'output \n",
    "    \n",
    "    logits = tf.layers.dense(inputs=last_rnn_output, units=n_outputs, name=\"logits\")\n",
    "    inference = tf.nn.softmax(logits, name=\"inference\")\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        #loss = cost(inference, y)\n",
    "\n",
    "        class_weights_tf = tf.constant(class_weights)\n",
    "        weights = tf.gather(class_weights_tf, y)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits, weights=weights)\n",
    "        #xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y) #ancienne version (sans poids)\n",
    "        #loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(tf.cast(logits, tf.float32), y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(\"./summary\", tf.get_default_graph())\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    n_batches_per_epoch = X_train.shape[0] // batch_size\n",
    "    print(\"Nombre de batchs par epoch =\", n_batches_per_epoch)\n",
    "    \n",
    "    best_loss = np.infty\n",
    "    checks_without_progress = 0\n",
    "    early_stopping = False\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, seq_len = get_next_batch_X(X_train, X_train_lengths, iteration, batch_size)\n",
    "                y_batch = y_train[iteration*batch_size:(iteration+1)*batch_size]\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch, seq_lengths: seq_len, dropout_in_placeholder: dropout_in, dropout_out_placeholder: dropout_out})\n",
    "                #print(sess.run(seq_lengths, feed_dict={X: X_batch, y: y_batch, seq_lengths: seq_len, dropout_in_placeholder: dropout_in, dropout_out_placeholder: dropout_out}).shape)\n",
    "                #sys.exit(0)\n",
    "                if (iteration+1)%10==0:\n",
    "                    print(\"Batch n°\", iteration+1)\n",
    "            #if (iteration+1)%100==0:\n",
    "            # attention l'évaluation du coût est faite en faisant une moyenne de moyennes,\n",
    "            # il faut donc que nb_examples_to_evaluate soit un multiple de batch_size_loss_eval\n",
    "            nb_examples_to_evaluate = np.amin([5000, X_train.shape[0], X_val.shape[0]])\n",
    "            batch_size_loss_eval = batch_size\n",
    "            #if(nb_examples_to_evaluate % batch_size_loss_eval != 0):\n",
    "                #print(\"WARNING: le nombre d'exemples de l'évaluation n'est pas un multiple de batch_size.\")\n",
    "\n",
    "            i=0\n",
    "            k=0\n",
    "            loss_train=0\n",
    "            while i < nb_examples_to_evaluate:\n",
    "                X_batch, seq_len = get_next_batch_X(X_train, X_train_lengths, k, batch_size_loss_eval)\n",
    "                y_batch = get_next_batch_y(y_train, k, batch_size_loss_eval)\n",
    "                temp = loss.eval(feed_dict={X: X_batch, y: y_batch, seq_lengths: seq_len})\n",
    "                #print(temp)\n",
    "                loss_train = loss_train + temp\n",
    "                i = i + batch_size_loss_eval\n",
    "                k = k + 1\n",
    "            #print(loss_train)\n",
    "            #print(k)\n",
    "            loss_train = loss_train / k\n",
    "            print(epoch, \"Loss training on\", nb_examples_to_evaluate, \"examples:\", loss_train)\n",
    "\n",
    "            i=0\n",
    "            k=0\n",
    "            loss_val=0\n",
    "            while i < nb_examples_to_evaluate:\n",
    "                X_batch, seq_len = get_next_batch_X(X_val, X_val_lengths, k, batch_size_loss_eval)\n",
    "                y_batch = get_next_batch_y(y_val, k, batch_size_loss_eval)\n",
    "                temp = loss.eval(feed_dict={X: X_batch, y: y_batch, seq_lengths: seq_len})\n",
    "                loss_val = loss_val + temp\n",
    "                i = i + batch_size_loss_eval\n",
    "                k = k + 1\n",
    "            loss_val = loss_val / k\n",
    "            print(epoch, \"Loss validation on\", nb_examples_to_evaluate, \"examples:\", loss_val)\n",
    "\n",
    "            if loss_val < best_loss:\n",
    "                save_path = saver.save(sess, \"./natural_language_classifier.ckpt\")\n",
    "                best_loss = loss_val\n",
    "                checks_without_progress = 0\n",
    "            else:\n",
    "                checks_without_progress += 1\n",
    "                if checks_without_progress >= MAX_CHECKS_WITHOUT_PROGRESS:\n",
    "                    print(\"Early stopping!\")\n",
    "                    early_stopping = True\n",
    "                    break\n",
    "                #if early_stopping:\n",
    "                    #break\n",
    "\n",
    "    return inference, X, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23292,)\n",
      "418\n",
      "Taille vecteur d'un mot = 300\n",
      "Nombre maximal de mots par texte (fixe) = 418\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[0].shape[0])\n",
    "n_steps = X_train[0].shape[0] #taille des textes (rendue fixe)\n",
    "n_inputs = X_train[0].shape[1] #taille des vecteurs représentant chaque mot\n",
    "print(\"Taille vecteur d'un mot =\", n_inputs)\n",
    "print(\"Nombre maximal de mots par texte (fixe) =\", n_steps)\n",
    "n_neurons = 1500\n",
    "activation = tf.nn.tanh\n",
    "dropout_in = 0.5\n",
    "dropout_out = 0.5\n",
    "\n",
    "n_outputs = 6\n",
    "class_weights = [1, 1, 1, 1, 1, 1] #poids de la fonction de coût\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "MAX_CHECKS_WITHOUT_PROGRESS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de batchs par epoch = 116\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "0 Loss training on 5000 examples: 2.382316017150879\n",
      "0 Loss validation on 5000 examples: 4.742771396636963\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "1 Loss training on 5000 examples: 1.4658703184127808\n",
      "1 Loss validation on 5000 examples: 3.019578094482422\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "2 Loss training on 5000 examples: 1.4886401605606079\n",
      "2 Loss validation on 5000 examples: 3.60544451713562\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "3 Loss training on 5000 examples: 0.69748379945755\n",
      "3 Loss validation on 5000 examples: 1.27174090385437\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "4 Loss training on 5000 examples: 0.589646817445755\n",
      "4 Loss validation on 5000 examples: 0.9447792100906373\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "5 Loss training on 5000 examples: 0.5802381408214569\n",
      "5 Loss validation on 5000 examples: 1.171145534515381\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "6 Loss training on 5000 examples: 0.4969486117362976\n",
      "6 Loss validation on 5000 examples: 1.0249522280693055\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "7 Loss training on 5000 examples: 0.4490309023857117\n",
      "7 Loss validation on 5000 examples: 0.8377081990242005\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "8 Loss training on 5000 examples: 0.3895909881591797\n",
      "8 Loss validation on 5000 examples: 0.8194437527656555\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "9 Loss training on 5000 examples: 0.3376565361022949\n",
      "9 Loss validation on 5000 examples: 0.5793503415584564\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "10 Loss training on 5000 examples: 0.330219686627388\n",
      "10 Loss validation on 5000 examples: 0.5826091134548187\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "11 Loss training on 5000 examples: 0.2968189537525177\n",
      "11 Loss validation on 5000 examples: 0.6646601116657257\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "12 Loss training on 5000 examples: 0.30290601193904876\n",
      "12 Loss validation on 5000 examples: 0.6731359767913818\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "13 Loss training on 5000 examples: 0.2537557643651962\n",
      "13 Loss validation on 5000 examples: 0.5032648265361785\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "14 Loss training on 5000 examples: 0.2558251541852951\n",
      "14 Loss validation on 5000 examples: 0.6074996554851532\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "15 Loss training on 5000 examples: 0.21123067080974578\n",
      "15 Loss validation on 5000 examples: 0.4627676546573639\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "16 Loss training on 5000 examples: 0.20820143222808837\n",
      "16 Loss validation on 5000 examples: 0.5024648082256317\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "17 Loss training on 5000 examples: 0.18338412463665008\n",
      "17 Loss validation on 5000 examples: 0.4694651436805725\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "18 Loss training on 5000 examples: 0.16726041316986084\n",
      "18 Loss validation on 5000 examples: 0.4134421646595001\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "19 Loss training on 5000 examples: 0.1516729152202606\n",
      "19 Loss validation on 5000 examples: 0.34680353105068207\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "20 Loss training on 5000 examples: 0.13226883739233017\n",
      "20 Loss validation on 5000 examples: 0.3691071081161499\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "21 Loss training on 5000 examples: 0.1342550951242447\n",
      "21 Loss validation on 5000 examples: 0.4113686954975128\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "22 Loss training on 5000 examples: 0.12083987176418304\n",
      "22 Loss validation on 5000 examples: 0.3733410775661469\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "23 Loss training on 5000 examples: 0.10608791872859001\n",
      "23 Loss validation on 5000 examples: 0.3542850506305695\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "24 Loss training on 5000 examples: 0.10365733057260514\n",
      "24 Loss validation on 5000 examples: 0.3538681811094284\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "25 Loss training on 5000 examples: 0.08195000350475311\n",
      "25 Loss validation on 5000 examples: 0.2995204657316208\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "26 Loss training on 5000 examples: 0.07567776575684547\n",
      "26 Loss validation on 5000 examples: 0.31378561556339263\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "27 Loss training on 5000 examples: 0.06015915311872959\n",
      "27 Loss validation on 5000 examples: 0.34643739998340606\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "28 Loss training on 5000 examples: 0.06000814616680145\n",
      "28 Loss validation on 5000 examples: 0.2977440804243088\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "29 Loss training on 5000 examples: 0.05188015766441822\n",
      "29 Loss validation on 5000 examples: 0.2806839090585709\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "30 Loss training on 5000 examples: 0.055592513009905815\n",
      "30 Loss validation on 5000 examples: 0.30727432668209076\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "31 Loss training on 5000 examples: 0.04326889730989933\n",
      "31 Loss validation on 5000 examples: 0.3479608255624771\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "32 Loss training on 5000 examples: 0.03703795913606882\n",
      "32 Loss validation on 5000 examples: 0.3190751576423645\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch n° 110\n",
      "33 Loss training on 5000 examples: 0.04027968749403953\n",
      "33 Loss validation on 5000 examples: 0.3346422588825226\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "34 Loss training on 5000 examples: 0.0336175437271595\n",
      "34 Loss validation on 5000 examples: 0.3297815990447998\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "35 Loss training on 5000 examples: 0.023683754466474055\n",
      "35 Loss validation on 5000 examples: 0.265144824385643\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "36 Loss training on 5000 examples: 0.02034807275980711\n",
      "36 Loss validation on 5000 examples: 0.2930563586950302\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "37 Loss training on 5000 examples: 0.020132123082876204\n",
      "37 Loss validation on 5000 examples: 0.2624404871463776\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "38 Loss training on 5000 examples: 0.01723468502983451\n",
      "38 Loss validation on 5000 examples: 0.29717893451452254\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "39 Loss training on 5000 examples: 0.014070943398401141\n",
      "39 Loss validation on 5000 examples: 0.30267200380563736\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "40 Loss training on 5000 examples: 0.01367259418591857\n",
      "40 Loss validation on 5000 examples: 0.32540948927402497\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "41 Loss training on 5000 examples: 0.009361347537487746\n",
      "41 Loss validation on 5000 examples: 0.3022582313418388\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "42 Loss training on 5000 examples: 0.01337987587787211\n",
      "42 Loss validation on 5000 examples: 0.3664649045467377\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "43 Loss training on 5000 examples: 0.00882564164698124\n",
      "43 Loss validation on 5000 examples: 0.3284255176782608\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "44 Loss training on 5000 examples: 0.007570991283282638\n",
      "44 Loss validation on 5000 examples: 0.37385053098201754\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "45 Loss training on 5000 examples: 0.005930729433894158\n",
      "45 Loss validation on 5000 examples: 0.3570092684030533\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "46 Loss training on 5000 examples: 0.0041566282091662285\n",
      "46 Loss validation on 5000 examples: 0.37334721386432645\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "47 Loss training on 5000 examples: 0.0065507885720580816\n",
      "47 Loss validation on 5000 examples: 0.3099086952209473\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "inference, X, seq_lengths = create_graph_RNN_and_train(X_train, y_train, size_texts_train, X_val, y_val, size_texts_val,\n",
    "                                                        n_steps, n_inputs, n_neurons=n_neurons, \n",
    "                                                        activation=activation, dropout_in=dropout_in,\n",
    "                                                        dropout_out=dropout_out, class_weights=class_weights,\n",
    "                                                        learning_rate=learning_rate, n_epochs=n_epochs, \n",
    "                                                        batch_size=batch_size, max_checks_without_progress=MAX_CHECKS_WITHOUT_PROGRESS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = np.array([[0,1,2,3,4,6],[1,0,1,4,5,8],[3,2,0,3,5,8],[10,7,5,0,2,7],[20,16,12,4,0,8],[44,38,32,19,13,0]])\n",
    "names = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes=['A1', 'A2', 'B1', 'B2', 'C1', 'C2'],\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print_confusion = True\n",
    "def cost(y_pred, y_true):\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    res = (1/y_true.shape[0]) * np.sum(np.multiply(costs, confusion))\n",
    "    res_by_level = np.sum(np.multiply(costs, confusion), axis=1)\n",
    "    \n",
    "    if print_confusion:\n",
    "        # Compute confusion matrix\n",
    "        cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        np.set_printoptions(precision=2)\n",
    "\n",
    "        # Plot normalized confusion matrix\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cnf_matrix, normalize=False, title='Confusion matrix')\n",
    "        plt.show()\n",
    "        plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')\n",
    "        plt.show()\n",
    "    return res, res_by_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./natural_language_classifier.ckpt\n",
      "Confusion matrix, without normalization\n",
      "[[2600   69   24    5    7    5]\n",
      " [  46 1658   82   19    4    9]\n",
      " [  15   13 1233   61    6   10]\n",
      " [   1    9   42  475    9   15]\n",
      " [  10    8   16   27   55    8]\n",
      " [   1    0    1    3    4    5]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[0.96 0.03 0.01 0.   0.   0.  ]\n",
      " [0.03 0.91 0.05 0.01 0.   0.  ]\n",
      " [0.01 0.01 0.92 0.05 0.   0.01]\n",
      " [0.   0.02 0.08 0.86 0.02 0.03]\n",
      " [0.08 0.06 0.13 0.22 0.44 0.06]\n",
      " [0.07 0.   0.07 0.21 0.29 0.36]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAEmCAYAAAAEH9kkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsnXd8FMX7x99PEkKHFBCSCyXUFFoKIEhHpSQEld4DKOpXilh+XzuKKCIixa5fKSLSwRCCBBWDUiQ0ESlCgAC5gGJUQISEHPP74y7hLgnJQS7cAfPmtS92dp6d+ezs5rmZ2dkZUUqh0Wg0mqvj5mwBGo1G4+poR6nRaDRFoB2lRqPRFIF2lBqNRlME2lFqNBpNEWhHqdFoNEWgHeVNiIi8LCKfW/Zrisg/IuLu4DxSReRuR6ZpR56PishvluvxLUY6/4hIHUdqcxYisldEOjhbx+2OdpQFYHESv4tIeatjD4pIkhNlFYhS6rhSqoJSyuRsLcVBREoBbwP3Wq4n43rTspx/xHHqHI+IzBWRSUXZKaVClVJJN0CSphC0o7w67sC44iYiZnQ5F001oAyw19lCXAER8XC2Bs0V9B/w1ZkKPCUiXgVFikhrEdkmImcs/7e2iksSkddEZBPwL1DHcmySiGy2NA3jRcRXRBaIyFlLGrWt0pgpIicscTtEpO1VdNQWESUiHiLSypJ2znZRRFItdm4i8oyIHBaRDBFZIiI+VukMEZFjlrjnCysYESkrItMs9mdEZKOIlLXExViai39brjnY6rxUEXlKRH62nLdYRMqISAPgV4vZ3yKy3vq68pTrg5b9eiKywZLOHyKy2MpOiUg9y35lEflMRE5b9L6Q88MlIrEW7W+JyF8iclREuhVy3aki8rRF/3kR+VREqonIVyJyTkS+ERFvK/ulInLKovF7EQm1HB8FDAL+L+dZsEr/vyLyM3Deck9zu0BEZI2ITLNKf5GIzC7sXmkchFJKb3k2IBW4G1gBTLIcexBIsuz7AH8BQwAPYIAl7GuJTwKOA6GW+FKWYylAXaAysA84aMnHA/gMmGOlYTDga4l7EjgFlLHEvQx8btmvDSjAI881lAI2AJMt4XHAj0AAUBr4CFhoiQsB/gHaWeLeBrKBu69SPu9ZrseAuebd2nJeA+A8cI8l//+zXLOnVbkmA/6WMtwPPFLQdRR0XZY8H7TsLwSex/xjXwZoY2WngHqW/c+AOKCiJc2DwEhLXCxwCXjIch2PAumAFPJc/Ii59msAfgd2AmEWDeuBCVb2Iyz5lgZmAD9Zxc3F8mzlSf8noAZQ1vpZtOxXt+TZCbOjPQJUdPbfy+2wOV2AK25ccZSNgDNAVWwd5RAgOc85W4BYy34SMDFPfBLwvFV4GvCVVbiH9R9SAZr+Appa9l+maEf5AbAacLOE9wOdreL9LE7CA3gJWGQVVx7IogBHaXFMF3K05Il7EViSx9YIdLAq18FW8W8CHxZ0HQVdF7aO8jPgYyCgAB0KqIfZ+WUBIVZxD1vdx1ggxSqunOXc6oU8F4OswsuBD6zCY4Avr3KulyXtypbwXAp2lCMKehatwr2AE8AfWP046K1kN930LgSl1C+Ync0zeaL8gWN5jh3DXMvI4UQBSf5mtX+hgHCFnIClibrf0mz7G3MttIo9ukXkYaADMFApddlyuBaw0tIk/huz4zRhrh35W+tVSp0HrvYypQrm2tPhAuJsysWS9wlsy+WU1f6/WF3zNfJ/gADJlqb+iKtoLYXtvcp7n3L1KKX+tewWpsmueygi7iLyhqWr4yxmh5ejqTAKem6sicf8A/CrUmpjEbYaB6EdZdFMwNw0s/7jSsfseKypibn2lMN1T8tk6Y/8P6Av4K2U8sJcsxU7z30V6KmUOmsVdQLoppTystrKKKWMwEnMzb2cNMphbvYXxB/ARcxdCHmxKRcREUu6xgJsi+K85f9yVseq5+wopU4ppR5SSvljriW+n9MvmUfrJWzvVd77VFIMBHpibplUxlxDhiv38GrPR1HPzWuYf+T8RGRAMTVq7EQ7yiJQSqUAi4GxVofXAA1EZKClw70f5n6+1Q7KtiLmPsLTgIeIvARUKuokEakBLAGGKqUO5on+EHhNRGpZbKuKSE9L3DIgWkTaiIgnMJGrPBuWWuJs4G0R8bfUnFqJSGlL3lEi0lnMw32eBDKBzdd09eZ8TmN2aIMteYzAyjmLSB8RCbAE/8LsYC7nScNk0fSaiFS0XPsTwOfXquc6qIj52jMwO/vX88T/BlzTWE8RaQcMB4YCw4B3RMRQ+FkaR6AdpX1MxNxvB4Ayj/GLxuwIMjDX/qKVUn84KL9EYC3mFw/HMNfgimqSAXTG3JReJlfefOcMt5kJrALWicg5zC8lWlquZy/wGPAF5trlX0BaIfk8BewBtgF/AlMw94X+ivkl1DuYa3M9gB5KqSw7rzsvDwFPYy7jUGwdbnNgq4j8Y7mucargsZNjMNdOjwAbLdd4I94Uf4b53hkxv7j7MU/8p0CIpSvky6ISE5FKljRHK6WMSqkfLGnMsdTcNSWIWDqINRqNRnMVdI1So9FoikA7So1GoykC7Sg1Go2mCLSj1Gg0miK4KT68F4+ySjwrOltGPpoF13S2hJsG/Vr21mDnzh1/KKWqOjJN90q1lMq+YJetunA6USnV1ZH528PN4Sg9K1K6YV9ny8jHhk2znC2hQNzdXM8tubmgJs21U7aU5P0irdio7At2/31f/Ok9u75OczQ3haPUaDS3MgIuPhOhdpQajca5CODiY+a1o9RoNM7HzaErmTgc7Sg1Go2T0U1vjUajKRrd9NZoNJpCEHSNUqPRaApHdI1So9FoikTXKDUajaYIdI1So9FoCsP133q7trpr5J7Wwexe+SK/xE3gqeH35Iuv6efNmg/HkLz4WRI/GYfhjitLdteo7k38+4+xa/kL7Fz+PDX9fPKdf718vW4t4U2CaRragLenTskXn5mZSezg/jQNbUDHtq04diwVgO3bkrmrZTh3tQyndYsw4uNWOkwTwLrEtTRrFETj4Pq8NfWNAnUNHdSfxsH1ad/mTo6lmnVlZGTQ7d5O3OFTkSfGjXa4piahDQkNqsfUNwvWNHhgP0KD6tG2dctcTQBTp0wmNKgeTUIb8vW6RK3LibquCcE8jtKezUncMo7SzU2Y8Uxfeo5+n7Bek+jTNYKgOtVtbCaPv58FCcm06DeZ1z/+ioljYnLj/vfqUKbP+5awXpNoO3gqp/865xBdJpOJJx8fw/K4BLbt+oVlSxdxYP8+G5vP5s7Gy9ub3XsP8tiYcUx43rzoY0hoIzZsSmbT1p2siFvDuDGPkp2d7TBdT4wbzcpVa9ixey9LFy9ifx5d8+Z8ipeXF3v2H2L02Md50aKrTJkyvDhhIq+/MdUhWqw1PT72MeLiv2LXz/tYumgh+/fZapo7+1O8vbzZeyCFMePG8/xz/wVg/759LF28iJ2797Jq9VrGjfkPJpNJ63KCrmvHUqO0Z3MSt4yjbN6oNodP/EGqMYNL2SaWJu4kukMTG5ugOn5sSP4VgA3bDhLdobHleHU83N1Yv/UAAOcvZHHh4iWH6Nq+LZk6desSGFgHT09PevXpR8LqVTY2CavjGDBoKAD3PdCbpKT1KKUoV64cHh7m3pGLmRdx5NIoZl31CKxj1tW7bz9Wx8fZ2KyOX8WgIcMAuP+B3iR99y1KKcqXL0/ru9pQukwZh+kB2JacTF0rTX369S9AU1yupgd69SZpvVnT6vg4+vTrT+nSpakdGEjduvXYlpysdTlB13XhJvZtzpLntJwdjP8dlUn77a/csPG3vzBUrWxjs+egkZ6dmgHQs1NTKlUoi0/l8tSveQd/n7vAorceZMvC//L64/c5bLabk+lGAgJyV4LF32Ag3WjMY5Oea+Ph4UGlSpX5M8O8rPa25K20CG9Mq8imzJj1fq7jLC7p6UYCagTkhg2GAE7m0ZVupT1HV0bG1Zb7dpAmq7IyGAIwFqSphpWmymZNRmP+c9PTHbMqrdZVwuSMo7xda5Qicp+IKBEJsjq21rLynKOWdrWbZ6evpG1EPbYs/C9tI+ph/O0vTKbLeHi4cVdYXZ6ZvpI2g6cSGFCFITF33mh5BdK8RUuSd+4haeNWpk2dwsWLF50tSaNxPCL2bU6ipF30AMxLhFov1D4VGOLojNJ/P0NANe/csKGaN8bTZ2xsTp4+Q/+n/kerAVOY8G48AGf+uYDxt7/5+WAaqcYMTKbLrPpuN82CauAI/PwNpKVdWWk23WjE32DIY+Ofa5Odnc3Zs2fw8fW1sWkYFEyFChXYt/cXh+jy9zeQduLKirRGYxp+eXT5W2nP0eWbR5cj8c9TVkZjGoaCNJ2w0nTGrMlgyH+uv79jlrzWukqa27iPUkQqAG2AkUD/nONKqW8Bx7wpsWL73mPUq1mVWv6+lPJwp0+XcBKSfrax8fUqn9vP9/SILsyL+zH33MoVy1LFuwIAHZo35MCRUw7RFRHZnCMpKaSmHiUrK4vlSxfTPaqHjU33qBgWLvgMgC9XLKN9+46ICKmpR3Nf3hw/doyDvx6gVq3aDtN1OOUQqUfNupYtWUxUdIyNTVR0DxbMnwfAyhXLaN+hk0P7SfMS2bw5KVaali5eVICmmFxNK5Yvo31Hs6ao6BiWLl5EZmYmqUePkpJyiOYtWmhdTtB1Xbh4jbIkx1H2BNYqpQ6KSIaIRCildth7soiMAkYBUKpCkfYm02XGT1lC/PuP4e4mzIv7kf1HTvHio1Hs3HechA17aBdZn4ljYlAKNu5M4fHJSwC4fFnx7NtfsubDMYgIu/YfZ/aKTdd10Xnx8PBg6vRZ3N+jGyaTiSHDhhMcEsqkiRMID4+ge3QMQ2NHMGrEUJqGNsDb24c5878AYMvmjUx/601KlSqFm5sbb898F98qjpng2cPDg2kz3qFndFdMJhNDY4cTEhLKq6+8RHh4JFE9Yhg2fCQPDh9K4+D6ePv4MG/+wtzzgxsEcu7sWbKysoiPj2NVQiLBwSHF1jR95rv0iOqCyWRiWOwIQkJDmfjyS4RHRBLdI4bYESMZETuE0KB6eHv7MH/BIgBCQkPp1acvYU1C8PDwYMas93B3d8xwEq3rBuDi4yhFKVUyCZv7IGcqpb4WkbFATaXUU5a4DsBTSqloe9JyK3eHcsWlIH7fopeCsBe9FMStQdlSskMpFenINN0q11ClW423y/Zi4pMOz98eSqRGKSI+QCegsYgowB1QIvK0KinPrNFobl5c/BPGkqrv9gbmK6VqKaVqK6VqAEeBtiWUn0ajuWm5fV/mDADyfm+3HBggIj8AS4HOIpImIl1KSINGo7lZuB1f5iilOhZwzDU79DQajXPRE/dqNBpNUbj+7EHaUWo0Gufj4i9ztKPUaDTORy9Xq9FoNIUguumt0Wg0RaOb3hqNRlM4JTmHgCPQjlKj0TgVQTtKjUajKRyxbC6MdpQajcbJiK5RajQaTVG4uqN07XfyGo3mtsDNzc2uzR5EpKuI/CoiKSLyTAHxNUXkOxHZJSI/i0j3IvVdxzVpNBqN45Br2IpKSsQdeA/oBoRgnogn74zSLwBLlFJhmFdfeL+odLWj1Gg0TkUsfZT2bHbQAkhRSh1RSmUBizCvtmCNAipZ9isD6UUlelP0UTYLrsmGTa43+dAd97zsbAkFYlz7krMl5KNMKdf8RM1VZ16/3ea3voY+yioist0q/LFS6mOrsAE4YRVOA1rmSeNlYJ2IjAHKA3cXlelN4Sg1Gs2tzTU4yj8csBTEAGCuUmqaiLQC5otII6XU5audoB2lRqNxOg58620ErNeaDrAcs2Yk0BVAKbVFRMoAVYDfr5ao7qPUaDTOxYEvc4BtQH0RCRQRT8wva1blsTkOdAYQkWCgDHC6sER1jVKj0TgdR9UolVLZIjIaSMS8qOFspdReEZkIbFdKrQKeBD4RkfGYX+zEFrXooXaUGo3GqQhi9xhJe1BKrQHW5Dn2ktX+PuCua0lTO0qNRuN8XHPwQS7aUWo0Gucirv8Jo3aUGo3G6WhHqdFoNEWgHaVGo9EUguhp1jQajcYOXNtP3loDzr9et5bwJsE0DW3A21On5IvPzMwkdnB/moY2oGPbVhw7lgrA9m3J3NUynLtahtO6RRjxcSsdquueFvXYvWAsvywcx1OD2uaLr1mtMmtmxJI89z8kzhqOoWql3Li4t4Zwcs2zLJ8yyKGaAL79OpEWYaFENglixrQ388VnZmYycuhAIpsEcU+H1hy3lNfxY6kYqlSkfasI2reK4Mmx/3GYpnWJa2nWKIjGwfV5a+obBWoaOqg/jYPr077NnRxLNWvKyMig272duMOnIk+MG+0wPda6moQ2JDSoHlPfLFjX4IH9CA2qR9vWLXN1AUydMpnQoHo0CW3I1+sSHa6raWgQjYLr89ZVdA0Z2J9GwfVpd9ed+XQ1Cq5P09Agh+u6Jiwvcxw0KUaJcMs4SpPJxJOPj2F5XALbdv3CsqWLOLB/n43NZ3Nn4+Xtze69B3lszDgmPG+eqi4ktBEbNiWzaetOVsStYdyYR8nOznaILjc3YcYT0fR8aj5hQ96lz92NCapd1cZm8mNdWLD2J1rEvs/rc5OY+PCVb/SnL9zEyEkrHKLFGpPJxP89MZYlK+LZvP1nVhRQXp/Pm42Xlxfbfz7Ao4+N45UXn8uNqx1Ylw1bdrBhyw6mzSpyliq7NT0xbjQrV61hx+69LF28iP15NM2b8yleXl7s2X+I0WMf50XLPSxTpgwvTpjI629MdYiWvLoeH/sYcfFfsevnfSxdtJD9+2x1zZ39Kd5e3uw9kMKYceN5/rn/ArB/3z6WLl7Ezt17WbV6LePG/AeTyeQwXePHjebL+DXszCmvvLrmfIqXtxe/7D/EmLGP88Jzz+TqWrZkMTt++oW41V/x+NjHHKbrenDkfJQlos9pOTuY7duSqVO3LoGBdfD09KRXn34krLb9cilhdRwDBg0F4L4HepOUtB6lFOXKlcPDw9wLcTHzokN/uZoHB3DY+CepJ//iUraJpd/uIbpNkI1NUO072LDzCAAbdh61iU/acYRz/2Y6TE8OO7cnE1inLrUt5XV/7358lRBvY/NVQjz9Bw0BIOb+XnxvKa+SwnwP6xFYx6ypd99+rI6Ps7FZHb+KQUOGAXD/A71J+u5blFKUL1+e1ne1oXSZMg7XtS05mbpWuvr061+ArrhcXQ/06k3SerOu1fFx9OnXn9KlS1M7MJC6deuxLTnZIbq2b7PVVVB5JcSvYnBOefW6Ul6r4+Po3befja7t2xyj67pw3CeMJcIt4yhPphsJCLjyLby/wUC60ZjHJj3XxsPDg0qVKvNnRgYA25K30iK8Ma0imzJj1vu5jrO4+FetSNrvZ3LDxtNnMVSpZGOzJ+UUPduZ5xbt2S6YSuXL4FOprEPyvxon09MxBARc0WkwcDI9f3n5W5dX5SvldfzYUTq0jqRHl05s2bTRIZrS040E1LiiyWAI4GSee5hudZ9z7mGGRVNJkZ7n2TIYAjAWpKuGbVllZGRgNOY/Nz097xwN16nLaLS5hwWlbbbJryvvNRX093Ijua2b3iJyn4goEQmyhJuJyBYR2WuZgr1fSeZ/LTRv0ZLknXtI2riVaVOncPHixRuW97PvJdK2WW22fPoobZvVxvj7GUyXXXc+wmrV/di9/whJm7fz6htTGTViCGfPnnW2LM1Nir1O8pZ1lJjnfdto+R/gX2CoUioU8zRHM0TEyxEZ+fkbSEu7Ml9nutGIv8GQx8Y/1yY7O5uzZ8/g4+trY9MwKJgKFSqwb+8vjpBF+ulzBNxROTdsqFoJ4x+2TuVkxjn6v7CIViM/YMIn3wJw5p+SddR+/v4Y09Ku6DQa8fPPX17p1uV1xlxepUuXzi23ZmERBAbW4XDKwWJr8vc3kHbiiiajMQ2/PPfQ3+o+59xD3zz30NH453m2jMY0DAXpOmFbVr6+vhgM+c/1z1PO163LYLC5hwWlbbbJryvvNRX093IjuW0dpYhUANpgnvutP4BS6qBS6pBlPx3z/G9Vr5rINRAR2ZwjKSmkph4lKyuL5UsX0z2qh41N96gYFi74DIAvVyyjffuOiAipqUdzX94cP3aMg78eoFat2o6QxfYDRuoF+FDLz4tSHu706dyYhI0HbGx8K5fLfQieHtyWeWt2OSTvwgiLaM6Rwykcs5TXymWL6dY92sama/doFi2YD8CqlctpaymvP06fzu34Tz16hMOHU6hdu06xNUVENudwyiFSj5o1LVuymKjoGBubqOgeLJg/D4CVK5bRvkOnEv8DimzenBQrXUsXLypAV0yurhXLl9G+o1lXVHQMSxcvIjMzk9SjR0lJOUTzFi0coisi0lZXQeXVPboHn+eU1/Ir5RUVHcOyJYttdEU2d4yu68HVHWVJjqPsCaxVSh0UkQwRiVBK7ciJFJEWgCdwuKCTRWQUMAqgRo2aRWbm4eHB1OmzuL9HN0wmE0OGDSc4JJRJEycQHh5B9+gYhsaOYNSIoTQNbYC3tw9z5n8BwJbNG5n+1puUKlUKNzc33p75Lr5VqhS/BACT6TLjpycQP20o7m5uzEvYyf7U07w4shM7DxhJ2PQr7cJqM3HUPSgUG3cf4/G3V+ee/827I2lQqwoVynqSsvxJHpkSxzfJKcXW5eHhwZRpM+lzXxQmk4mBQ2IJCgll8qsv0yw8gm5RPRg8bASPPhhLZJMgvLy9+d/cBQBs3vQDb0x6hVKlPHBzc2PazPfw9vFxiKZpM96hZ3RXTCYTQ2OHExISyquvvER4eCRRPWIYNnwkDw4fSuPg+nj7+DBv/sLc84MbBHLu7FmysrKIj49jVUIiwcF515W6Pl3TZ75Lj6gumEwmhsWOICQ0lIkvv0R4RCTRPWKIHTGSEbFDCA2qh7e3D/MXLAIgJDSUXn36EtYkBA8PD2bMeg93d8csi+Hh4cHbM94hJqorpssmhg4bnl/X8JGMjB1Ko+D6eHv78NnnC3N1PdC7D+FNQ/FwN1+fo3RdFy4+jlJK6i2miKwGZiqlvhaRsUBNpdRTljg/IAkYppT6sai0wiMi1YZNTnwjdxX0mjn2o9fMuTZcdc2ccp5uOxywFIMNpavVV4ZBM+2yPTo9yuH520OJ1ChFxAfoBDQWEYV5Ak0lIk8DFYEE4Hl7nKRGo7m1EXHdH6wcSqqPsjcwXylVSylVWylVAzgKtAVWAp8ppZaVUN4ajeam4vZ96z0As0O0ZjkwD2gHxIrIT5atWQlp0Gg0Nwki9m3OokSa3kqpjgUcmwW43uLcGo3G6ejZgzQajaYwnFxbtAftKDUajVMRXP9ljnaUGo3G6egapUaj0RSB7qPUaDSaQrgZxlFqR6nRaJyMXjNHo9FoisTF/aR2lBqNxvnoGqVGo9EUhh5HqdFoNIUj6BqlRqPRFImL+0ntKDUajfPRw4M0Go2mMEQ3vR2Guwv+4mR8+4qzJRSIb8cXnC0hH8Z1rllWZT1dc+b12wlzH6WzVRTOTeMoNRrNrYoecK7RaDRF4uJ+UjtKjUbjfHSNUqPRaApDDzjXaDSawrkZBpyX1OJiGo1GYzdubmLXZg8i0lVEfhWRFBF55io2fUVkn4jsFZEvikpT1yg1Go3TcVSNUkTcgfeAe4A0YJuIrFJK7bOyqQ88C9yllPpLRO4oKl1do9RoNM7FzqVq7fSlLYAUpdQRpVQWsAjomcfmIeA9pdRfAEqp34tKVDtKjUbjVMQyjtKeDagiItuttlF5kjMAJ6zCaZZj1jQAGojIJhH5UUS6FqVRN701Go3TuYaW9x9KqchiZucB1Ac6AAHA9yLSWCn1d2EnaDQajVNxc9xbbyNQwyocYDlmTRqwVSl1CTgqIgcxO85tV9XnKHWuwLrEtTRrFETj4Pq8NfWNfPGZmZkMHdSfxsH1ad/mTo6lpgKQkZFBt3s7cYdPRZ4YN/q20XVPy/rsXvg4vyx+gqcGt8sXX7OaF2tmjiB53hgS3xmJoWolAJrU9yPpo4fZ8flYkueNoXfnxg7T9O3XibQICyWySRAzpr2ZLz4zM5ORQwcS2SSIezq05vixVACOH0vFUKUi7VtF0L5VBE+O/Y/DNIH5HjYNDaJRcH3eerPgezhkYH8aBden3V1X7iHA1CmTaRRcn6ahQXy9LtEldGVkZND1nk5U9a7I+BJ4tq4VB/ZRbgPqi0igiHgC/YFVeWy+xFybRESqYG6KHyks0VvGUZpMJp4YN5qVq9awY/deli5exP79+2xs5s35FC8vL/bsP8TosY/z4vPmkQNlypThxQkTef2NqbeNLjc3YcaTPej55DzCBs2kz91NCKpd1cZm8uiuLFi7ixbD3uH1Od8x8ZF7Afj3YhYjX11GxOBZ9HxyLm+OjaJyhTLF1mQymfi/J8ayZEU8m7f/zIqliziQp6w+nzcbLy8vtv98gEcfG8crLz6XG1c7sC4btuxgw5YdTJv1frH1WOsaP240X8avYWfOPdxnq2vunE/x8vbil/2HGDP2cV54znwP9+/bx7Ili9nx0y/Erf6Kx8c+hslkcrquMmXK8NLLE3l9iuOfrWtFLLMH2dlHWShKqWxgNJAI7AeWKKX2ishEEYmxmCUCGSKyD/gOeFoplVFYureMo9y+LZk6desRWKcOnp6e9O7bj9XxcTY2q+NXMWjIMADuf6A3Sd99i1KK8uXL0/quNpQuU/w/9ptFV/PgAA6n/Ulq+l9cyjax9NufiW4bbGMTFHgHG3aYf2g37DySG59yIoPDaebn6uQf5zj91z9U8SpfbE07tycTWKcutQPNZXV/7358lRBvY/NVQjz9Bw0BIOb+XnyftB6lVLHzLozt25KpW8Q9TIhfxeCce9jryj1cHR9H7779KF26NLUDA6lbtx7btyU7XVfOs1WmBJ6t68HdTeza7EEptUYp1UApVVcp9Zrl2EtKqVWWfaWUekIpFaKUaqyUWlRUmld1lCJSqbDNzuu/YaSnGwmoEZAbNhgCOGk05rcJMHdfeHh4UKlSZTIyCv0huWV1+VetRNrvZ3LDxt/PYqha2cZmz6FT9GwfAkDP9iFUKl8Gn0plbWwigwPwLOVmc6JZAAAgAElEQVTOEeOfxdZ0Mj0dQ8CVsvI3GDiZbsxn429dVpUr86elrI4fO0qH1pH06NKJLZs2FltPDulGo40ugyGA9Dy6zDa2ujIyMmzubc41pRvzdpndeF2uhgOb3iVCYS9z9gIK8xdGOeSEFVCzsIRFxATssdibgNFKqc2WuLXAncBGpVT0davXlCjPvvcV05/oweDu4Wz6KRXj72cwXb5Se6vuW5FPX+rNQ5OWl3itriiqVfdj9/4j+Pj68tOuHQzp35tN23ZTqZLL/aZr8iCYhwi5Mld1lEqpGleLs5MLSqlmACLSBZgMtLfETQXKAQ8XM49c/P0NpJ1Iyw0bjWn4GQz5bdJOYAgIIDs7m7Nnz+Dr6+soCTeVrvTTZwm440oN0nBHJYynz9jYnPzjHP2fM3/dVb6sJ/d1COXMPxcBqFiuNCumDuXlj74mee8JHIGfvz/GtCtllW404udvyGeTnnYCg8FSVmfO4OPri4hQunRpAJqFRRAYWIfDKQcJCy/uSBJzLdBal9GYhn8eXWabEwQEXNHl6+ube2+tr8nfkHdY343X5Wq44LzcNtjVRyki/UXkOct+gIhEXGM+lYC/cgJKqW+Bc9eYRqFERDbncMohUo8eJSsri2VLFhMVHWNjExXdgwXz5wGwcsUy2nfoVOIf47uqru0HjNQL8KWWnzelPNzp07kJCRsP2Nj4Vi6Xq+PpIe2Zl7ADgFIe7iyePIgv1u5iZdJeh2kKi2jOkcMpHEs1l9XKZYvp1t22wdG1ezSLFswHYNXK5bRt3xER4Y/Tp3NfkqQePcLhwynUrl3HIboiIpuTUsQ97B7dg89z7uHyK/cwKjqGZUsWk5mZSerRo6SkHCKyeQun63Ip7HyR40zdRY6jFJF3gVJAO+B14F/gQ6B5EaeWFZGfgDKAH9DpWoRZRtyPAqhRs9BWPmDuf5k24x16RnfFZDIxNHY4ISGhvPrKS4SHRxLVI4Zhw0fy4PChNA6uj7ePD/PmL8w9P7hBIOfOniUrK4v4+DhWJSQSHBxyLZJvKl0m02XGT48n/u1Y3N2Feat3sv/o77z4YGd2HjCSsPEA7cICmfjIvSgFG3en8vg08yiLXp0a0aZZbXwql2Nw93AARr22nJ8PnSyWJg8PD6ZMm0mf+6IwmUwMHBJLUEgok199mWbhEXSL6sHgYSN49MFYIpsE4eXtzf/mLgBg86YfeGPSK5Qq5YGbmxvTZr6Ht49P8QrJStfbM94hJqorpssmhg4bTkhoKBNffonwiEiie8QQO3wkI2OH0ii4Pt7ePnz2ufkehoSG8kDvPoQ3DcXD3YPpM9/F3d0xy08URxdAUH2rZ2tVHPEJiQSHFP/Zuh5czXfnRYrqWxKRnUqpcBHZpZQKsxzbrZRqWsR5/yilKlj2WwH/AxopS4Yi0gF4yp4+yvCISLVxy1XHgmryoNfMsR+9Zs61Uc7TbYcDvoyxwbt2iOr44ny7bFc+GOnw/O3Bnqb3JRFxw/wCBxHxBS5fSyZKqS1AFaBqUbYajeb242Z+653De8ByoKqIvAL0Ba6peiAiQYA74HrjEjQajVMRuQXW9VZKfSYiO4C7LYf6KKV+sSPtnD5KMI8AGKaUMgGIyA9AEFBBRNKAkUopx37bpdFobhoc+K13iWDvpBjuwCXMzW+73pQrpa7a+aOUamtnvhqN5jbAtd2kHU5PRJ4HFgL+mGfi+EJEni1pYRqN5vbhph8eBAwFwpRS/wKIyGvALswDyDUajaZYCK4/4NweR3kyj52H5ZhGo9EUHyfXFu3hqo5SRKZj7pP8E9grIomW8L0UMsGlRqPRXCsu7icLrVHmvNneCyRYHf+x5ORoNJrbkZu2RqmU+vRGCtFoNLcnAnbPNeks7PnWuy7wGhCC+bttAJRSDUpQl0ajuY1wbTdp35jIucAczNfSDVgCLC5BTRqN5jZCxDzg3J7NWdjjKMvlfDWjlDqslHoBs8PUaDQah3ArfOudaZkU47CIPIJ56ceKJStLo9HcTty0L3OsGA+UB8Zi7qusDIwoSVEajeb2wsX9pF2TYmy17J4DhpSsHI1Gc7shOLf/0R4KG3C+EssclAWhlHqgRBQVpAXXnIYp23RN03LeMI6umeBsCfkw9HHcOtuO5PTK0c6WUCCXnbtW243lJp9m7d0bpkKj0dzW2DUlmRMpbMD5tzdSiEajuT0Rbo2XORqNRlOiuHjLWztKjUbjfG4ZRykipZVSmSUpRqPR3H6YB5O7tqe0Z4bzFiKyBzhkCTcVkXdKXJlGo7ltcBP7Nqfps8NmFhCNZQVFpdRuoGNJitJoNLcXt8InjG5KqWN5qsamEtKj0WhuMwTwcPGmtz2O8oSItACUiLgDY4CDJStLo9HcTri4n7TLUT6KufldE/gN+MZyTKPRaIqNOHkKNXuw51vv34H+N0CLRqO5TXFxP2nXDOefUMA330qpUSWiSKPR3Ha4+jhKe956fwN8a9k2AXcALjmecl3iWpqENiQ0qB5T33wjX3xmZiaDB/YjNKgebVu35Fhqam7c1CmTCQ2qR5PQhny9LtGhur5et5awxsE0DWnAtKlTCtQ1bHB/moY0oGPbVrm61n/zNW1bNadlRFPatmrOhu/WO1TX+m8SaRPZiFZhwbwzfWqBuh4ePohWYcF079yGE8fMui5dusTYR0bSsXU4bVs0YdbbbzpM0z0Rtdj98RB++d9QnuoTkS++RtUKrJ38AFveGUDyewPpElkrN65RbV+SpvVhxweD2Pb+QEqXcneYLle9h9+sW0tEk2CahTbg7avoih3cn2ahDejUthXHLPdwx7Zk2rQMp03LcO5qEUZ83EqH6roWzOt6u/YM5/Y0vW2WfRCR+cDGElN0nZhMJh4f+xgJX32NISCANnc2Jzo6huCQkFybubM/xdvLm70HUliyeBHPP/dfPv9iMfv37WPp4kXs3L2Xk+npdO96N3v2HcTdvfh/aCaTiSfHjSEuIRFDQADt72pJVHQPgoKv6Pps7my8vLzZve8gy5Ys4qUXnmHe54vwrVKFJcvj8PP3Z9/eX7ivRzcOHjlRbE05up57ahyLv1yDn38A3Tq25t5u0TQMCs61WTh/DpW9vNiyaz9fLl/CpJef56M5C4j/cjlZWZl8t3kn//77L+1bNuP+Xn2pUat2sTS5uQkz/tOBqOdXYvzjHzbO6MfqH49y4MSfuTb/7d+C5T8c4pM1ewiq4cOXE2MIGj4Xdzdh9tNdGPnWOvYc/QOfimW45KDZnVz5Hj75+Bi+TEjEYAigY5uWdC9Il7c3P+0165rw/DPM/XwRwaGNSNqUjIeHB6dOnuSulmF0i+qBh4dzPtZz9ab39UzaEQhUc7SQ4rItOZm6desRWKcOnp6e9OnXn9XxcTY2q+PjGDRkGAAP9OpN0vpvUUqxOj6OPv36U7p0aWoHBlK3bj22JSc7RNf2bcnUqVs3V1evPv1YHb/KxiYhPo6Bg4cCcN8DvUn6bj1KKZo2C8PP3x+A4JBQLl64QGamYyrzu3Zso3adutSqbdbVs1dfEtfE29isXRNP3wHmKUijez7ADxu+QymFiPDv+fNkZ2dz8eIFPD1LUaFSpWJrat6gGofT/yb11FkuZV9m6feHiG5Vx8ZGKUWlcp4AVC7vycmM8wDcHV6TX47+wZ6jfwDw57mLXHbQXGWueg935OgKNOt6oE8/Elbb6lqzOo6Bg67o2pBk1lWuXLlcp3gx86Jzv4yxc7C5Sw84F5G/RORPy/Y38DXwbMlLuzbS040EBNTIDRsMARiNxvw2Ncw2Hh4eVKpcmYyMDIzG/Oemp9uee72cTDdisEnbwMn0vLrSc/P38PCgciWzLmviVi6nabNwSpcu7RBdp06mYzBc0eXnb+DUSWM+G39DQK6uSpUq8eefGUT3fIBy5cvTtGEtIhvV45Ex4/H29im2Jn/fCqT98U9u2PjHPxh8y9vYvLZgK/07NSTlsxGsfCWGJz5MAqC+wRsFrHq1J5tn9eeJ3uHF1pODq97D9IJ05XnmT6an59qY72Fl/rTo2p68lZbhjWkd2ZTps953Xm0ScBexa3MWhTpKMf/MNAWqWjZvpVQdpdSSohIWEZOI/CQiu0Vkp4i0thxvJiJbRGSviPwsIv0ccSG3Mvv37eWl559l5rsfOFsKYK6Nurm789OBVJJ3/8pH787gWOqRG5J33w4N+fzr/dQbOpv7J6zi06e6IAIe7kLrED+GT02k89PLiGlVlw5NA26IJntwtXsIENmiJVt37uG7jVt5e+oULl686DQtjqxRikhXEflVRFJE5JlC7HqJiBKRyCL1FRaplFLAGqWUybJdS1vmglKqmVKqKeYa6GTL8X+BoUqpUKArMENEvK4h3QLx9zeQlnal78doTMNgMOS3OWG2yc7O5uyZM/j6+mIw5D/X39/23OvFz9+A0SZtI37+eXX55+afnZ3NmbNmXQDGtDQG9O3FR5/OpU7dug7RBFDdzx+j8Yquk+lGqvsZ8tmkG9NydZ09exYfH19WLltEx873UqpUKapUvYPmLVuze9fOYmtKz/iHgCoVcsOGKhUwWprWOQy7N4TlPxwCYOuBU5Qp5U6VSmXNfZq/pJNx9iIXMrNZuz2VsHp3FFsTuO499C9IV55n3s/fP9fGfA/P4GPRlUPDoGDKV6jAvr2/OEzbtSIidm12pOMOvId5pdgQYICIhBRgVxEYB2zNG1cQ9vRR/iQiYfYkVgiVgL8AlFIHlVKHLPvpwO+Ya6vFIrJ5c1JSDpF69ChZWVksXbyIqOgYG5uo6BgWzJ8HwIrly2jfsRMiQlR0DEsXLyIzM5PUo0dJSTlE8xYtiisJgIjI5hxOScnVtXzpYqKie9jYdI+O4YvPPwPgyxXLaN+hIyLC33//Te/7e/DKpNdp1fouh+jJoVl4JEcPp3A81awrbvkSunSLtrHp0i2aJQvnA7A6bgVt2nVARDAE1GTT90kA/Hv+PDu2b6Ve/YbF1rT94G/U8/eiVrVKlPJwo0+7+iT8aFtTPXH6HB2amZuSDWt4U8bTndNnLvD1zuOE1valbGkP3N2Eto0M7D/+Z0HZXDOueg/Dc3RZ7uGKpYvpHpVHV1QMXyy4oqtde7Ou1NSjZGdnA3D82DEO/XqAWsV8GXe9mN96O6xG2QJIUUodUUplAYuAngXYvQpMAeyqRhe2Zo6HUiobCAO2ichh4LzlupRSqqhOoLIi8hNQBvADOhWQRwvAEzhcQNwoYBRAjZo1i74QDw+mz3yXHlFdMJlMDIsdQUhoKBNffonwiEiie8QQO2IkI2KHEBpUD29vH+YvWARASGgovfr0JaxJCB4eHsyY9Z5D3njn6Hprxizu69GNyyYTQ4YNJzgklEmvTCAsIoKo6BiGxo7goRFDaRrSAG8fH+Z89gUAH3/wHkcOpzDl9UlMeX0SAHGr11L1juLXlDw8PHh96gwG9IrGZDLRf3AsDYNDePO1V2gaFk6X7j0YMGQ4Yx4eTquwYLy8ffhwttlpDn/wER5/7CHa39kMpRT9Bw0lpFHjYmsyXVaM/yCJ+Ek9cXdzY966vew//icvDm7JzkO/k7D1KM98spH3x3VizH3NUAoeevsbAP7+J5NZK3excUY/lILE7ams3ZZabE3g2vfwremzeKBHN0wmE4Mtul6bOIGw8Ai6R8cwJHYEo0YMpVloA7y9fZg936zrx80bmf7Wm5QqVQpxc2PazHfxrVKl2Jqui2ub8KKKiGy3Cn+slPrYKmwArIcVpAEtbbITCQdqKKUSRORpuyRerTUtIjuVUuEiUmBbQSmVz7nlOf8fpVQFy34r4H9Ao5zmu4j4AUnAMKXUj4WlFRERqTZt3V6YiVNw1cXF/rmY7WwJ+Qgc8KGzJRSIXlzs2qhc1n2HUqrIPr1roUZQY/XkJ6uKNgTGt6tTaP4i0hvoqpR60BIeArRUSo22hN2A9UCsUipVRJKAp5RShTqYwl5zCRTtEO1BKbVFRKpgbmL/LiKVgATg+aKcpEajubXJaXo7CCNQwyocYDmWQ0WgEZBk6fOsDqwSkZjCnGVhjrKqiDxxtUil1Nv2qAYQkSDAHcgQEU9gJfCZUmqZvWloNJpbFweO/NkG1BeRQMwOsj8wMCdSKXUGyO1jcESN0h2ogKVmeR3k9FFiSWOYUsokIgOAdoCviMRa4mOVUj8VlIhGo7m1ERw3RlIplS0io4FEzD5stlJqr4hMBLYrpexr4+ehMEd5Uik18XoSBVBKFfg2RCn1OfD59aar0WhuMRz81Y1Sag2wJs+xl65i28GeNIvso9RoNJqS5maej7LzDVOh0WhuWwTXnxTjqo5SKeWY0boajUZTBDdzjVKj0WhuCC7uJ7Wj1Gg0zkW4vvkebyTaUWo0GuciOHc+TDvQjlKj0TiVnPkoXRntKDUajdNxbTepHaVGo3EBXLxCqR2lRqNxNvZNyutMtKPUaDRORb/11mg0GjvQNUoHkH1Z8df5LGfLyIe7M9fPLITzmSZnS8jH9+8Oc7aEApm8PsXZEgpkXJtAZ0u4objmX9IVbgpHqdFobmH0OEqNRqMpHD2OUqPRaOzAtd2kdpQajcYFcPEKpXaUGo3GuZiHB7m2p9SOUqPROB1do9RoNJpCEUTXKDUajaZwdI1So9FoCkFEDw/SaDSaInFxP6kdpUajcT6u3kfp6pN2XBPffZNIm8hGtA4L5p3pU/PFZ2Zm8vDwQbQOCyaqcxtOHEsF4NKlS4x7ZCSdWofTrkUT3nn7TYfqWv91Iq3DQ2nZNJhZBaSdmZnJQ7EDadk0mK4d7+K4RRfA3l9+pnvntrRr0ZT2d4Zx8eJFh+nasH4d97RuSqeWjfhw1lv54pO3bCTm7lY09K/IV/Erc48bTxwn5u5W9OjUkq7tIvhi3icO07Rlwzf0vjuSBzqGMe/D6fniF3z6Lv26tGRg99b8Z3AMJ43HATi472dG9L6Hfl3vZGD31ny9eoXDNAEc3v49Hz7UhQ9G3sPmJR9f1e7AxkRe796Qkwf32Bw/83s6Ux8I48flnzpUl6s+W9eCAG5i3+YsbhlHaTKZeO6pcSxYtoqkrbuJW7aYgwf229gsnD8HLy8vNu/az0P/Gcukl58HIP7L5WRmZbJ+807WJv3I/Dn/y3WijtD1zJPj+GJ5PD9s283KZYv59cA+G5svPpuDl5c3W3fv5+HHxvLqhOcAyM7O5rGHYpk6412+T97NyoRvKFWqlMN0vfzMeD794kvW/rCT1SuXcuhX2/LyN9TgzZkf0+OBfjbHq1arztKEJOLXb2X5Vxv46J1p/HYq3SGa3nz5KWbOXsbixK0kxi/jyKEDNjYNQ5ow78vv+GLNZjp168k7b0wAoHTZcrw89UMWr/2RmXOW8/akZzl39u9iawK4bDKR+P5E+k38H6M+TGDfhtWcPp5/Mo3Mf/9hW9xn+Ddsmi/um0/eoG5kW4foycFVn63rQez85yxuGUe5a8c2atepS63adfD09KRnr74krom3sUlcE0+fAUMAiO75ABs3fIdSChHh3/Pnyc7O5uLFC3h6lqJCpUoO0bVz+zYC69SldqBZ1329+rI2wVbX2oR4+lp09bivFxuTzLqSvv2akNDGhDY2/+H5+Pri7u7uEF27d26nVmBdatYOxNPTk6j7evPN2tU2NgE1axEU2hg3N9vHxNPTk9KlSwOQlZnJ5cuXHaJp7+4dBNSqg6FmbUp5enJvdC++/2aNjU1kq3aUKVsOgMbNIvnd4qBrBdajZmBdAKpW88Pbtwp/ZWQ4RFf6wZ/x9q+Ft18N3Et5EtIuikNbvs1n9/38mbTq8xAenqVtjv+6+Ru8qhuoUrO+Q/Tk4KrP1vUgYt/mLG4ZR3nqZDr+hhq5YT9/AydPGguwCQDAw8ODSpUq8eefGUT3fIBy5cvTrGEtmjeqxyNjxuPt7eMgXUb8AwJyw/7+Bk6l29a+Tp40Ygi4oqtipcr8+WcGh1MOISL0uy+Ku9u24N0Z+ZvH18tvp9Lx8zfkhqv7G66pVphuTCOqQwvahjdg1OgnqFbdv9iaTv92kmp+VzTdUd2f07+dvKr9qqWf06r93fmO7929g+xLlwio5Zipys5l/EalKtVzwxWrVONcxm82NqdS9nL29Cnqtehgczzrwnl+XPYJbQeOdogWmzxd9Nm6Hly9RlmiL3NEpDowA2gO/A38BjwOzALuBDYqpaJLUoM97NqxDXd3d3YdSOXM339xX7dOtO3QiVq16zhVl8mUzdYfN5OYtJmyZcvRu0cXmjQLp12HTk7VBeBvCCAhKZnfTqXz6LB+dIu+nyp3VLth+X/15WL279nFh18k2Bz/4/dTTHjyYSZM/SBfTbikUJcv880nbxD9xOR8cT8seJfm9w3Ds2z5G6LFXlzp2crpo3RlSuxJEvMEcyuBJKVUXaVUBPAsUA2YCgxxZH7V/fxJN57IDZ9MN+JnVTu5YpMGmPtozp49i4+PLyuXLaJj53spVaoUVareQfOWrdm9a6eDdBlIT0vLDaenG6nub1v78vMzYEy7ouvc2TP4+Pji52+gVes2+PpWoVy5ctx9b1f27N7lEF3VqvtzMv1KjftUuvG6aoXVqvvTICiEbVs3F1tT1Wp+/GbVCvj9VDpVq/nls0velMSc96fx1kcL8Sx9pZn7z7mzjH+wL48++SKNw5oXW08OFX2rcfaPU7nhc3/8RkXfKz8KmRfOc/rYQRb8dyjvxXbCeOAnlk58lJMH92D8dTffzX6L92I7sS1uHpsXf8T2+M8dostVn61rRgQ3OzdnUZI/uR2BS0qpD3MOKKV2K6V+UEp9C5xzZGbNwiM5ejiF46lHycrKIm75Eu7tZltZvbdbNEsXzgdgddwK2rTrgIhgCKjJxu+TAPj3/Hl2bt9KvfoNHaIrLCKSI0dSOGbR9eXyJXTpbqurS/dollh0xX+5nDbtzbo6dr6X/ft+4d9//yU7O5vNm36gQcNgh+hqEhbBsSMpnDiWSlZWFglfLqNzlyi7zj2ZnsbFCxcAOPP3X2xP3kKdusXvfwtpEs6J1MMYT6RyKSuLdauX07ZzNxubX/fuZvILj/PWRwvxqVI19/ilrCz+79HBdL+/P5279Sy2Fmv8GzTmr/RU/j51AtOlLPZ9n0D9O6/UvMqUr8j4RVt5bO56Hpu7HkNQM/q89AF+DRozdOoXuceb9xxG634PE9ljsEN0ueqzdT2InZuzKMmmdyNgRwmmb4OHhwevTZ3BwF7RmEwm+g+OpWFwCG++9gpNw8Lp0r0HA4YMZ+zDw2kdFoyXtw8fzDY/QMMffITxjz1EhzuboZSi36ChhDRq7DBdk6fOoP/9UZhMlxkwZBhBwaFMmfQyTcMj6Nq9BwOHDmf0qFhaNg3Gy9ubj+aYaxxe3t488tg4unZoBSLcfW9X7una3WG6Jkx+m+H9YzCZTPQZMJQGQSHMmDKRRk3DubtrND/v2s6jw/tz9u+/Wb9uDTOnTmLt9zs4fOhXJk94FhFBKcWDj46jYUgjh2h6esJUxsb24vJlEz16D6Zug2A+mv4awY3DaHd3d2a98RIXzp/n2THmpSWq+wcw7eNFfLNmJbu2bebM33+yevkXAEx4830ahDQpti43dw/uffQlFr3wIJcvm2h6by+q1qrPhvkz8avfiAZ3di52HteDqz5b14q56e3abW9RSpVMwiJjgUCl1PirxHcAnrpaH6WIjAJGARhq1IzYtudQiegsDq66Zs65C9nOlpCPP/9xvTWPAL789beijZyAq66ZU62S5w6lVKQj0wxuHKbmrPzOLttW9b0dnr89lGTTey8Qcb0nK6U+VkpFKqUifX2rOFCWRqNxOVy87V2SjnI9UNpSMwRARJqIiGNH3Wo0mpseVx8eVGKOUpnb9PcDd4vIYRHZC0wGTonID8BSoLOIpIlIl5LSodFoXB9XH3BeouMolVLpQN8ConStUqPR5OKavf1X0LMHaTQapyK4/rret8wnjBqN5ibFzma3vb5URLqKyK8ikiIizxQQ/4SI7BORn0XkWxGpVVSa2lFqNBqn46iX3iLiDrwHdANCgAEiEpLHbBcQqZRqAiwDipxXUTtKjUbjfBw3PKgFkKKUOqKUygIWATafaimlvlNK/WsJ/ggEUATaUWo0Gidj7+AgAagiItuttlF5EjMAJ6zCaZZjV2Mk8FVRCvXLHI1G43Su4V3OH476MkdEBgORQPuibLWj1Gg0TsXBH90YgRpW4QDLMds8Re4GngfaK6Uyi0pUN701Go3zcVwf5TagvogEiogn0B9YZZOVSBjwERCjlPrdnkR1jVKj0TgdR80epJTKFpHRQCLgDsxWSu0VkYnAdqXUKszz4VYAllrGbx5XSsUUlq52lBqNxuk4cri5UmoNsCbPsZes9vOvH1IE2lFqNBrn4uxZee1AO0qNRuN0nDkzkD1oR6nRaJyK+VtvZ6sonJvGUbriVPGVyjpvwfjCKKFJ64vFX+ddc4bzNjW9nC2hQBIOXH2Z3lsR1/vrtuWmcZQajeYWxsU9pXaUGo3G6eg+So1GoykCF12nLxftKDUajfPRjlKj0WiujnkYpWt7Su0oNRqNc3HywmH2oB2lRqNxOi7uJ7Wj1Gg0LoCLe0rtKDUajZMR3Uep0Wg0hSHo4UEajUZTNC7uKG+pGc7Xf5PIXRGh3NksmHfezr8CZWZmJqNiB3Jns2C6dbqL48dSAVi+5As6t4nM3fy8SvPLzz85TNe6xLU0CW1IaFA9pr75RoG6Bg/sR2hQPdq2bsmx1NTcuKlTJhMaVI8moQ35el2iwzSBa5bX5g3f0KtzJPd3DGPuB9PzxS/437v0vbclA7q15tFBMZw0Hs+NGxPbi45NazJ+ZD+HaLFm+8b1PBTdmpHdWrLkf7Pyxa+Y9yEPx7TlP/d34NmRvfgt/cr6VrPffpVH72vHo/e1Y8NXXzpU154tSUKF3PgAABKqSURBVDzbuyPPPNCOhHnv54v/bvnnvDjgXiYM6sbrD/XCeORgbtyJQ/t5bcR9vNDvbl4ccC+XMi86VNu1cA2LizmFW8ZRmkwmnn1yHF8si+f75N2sXL6YXw/ss7H54rM5eHl58+NP+3n4P2OZNOE5AHr1Hci3G7fz7cbtvPvRHGrWCqRRk2YO0/X42MeIi/+KXT/vY+mihezfZ6tr7uxP8fbyZu+BFMaMG8/zz/0XgP379rF08SJ27t7LqtVrGTfmP5hMJofpcrXyMplMvDnhKWbOWcaSxK2si1/GkUMHbGwahjbhs7jvWPjVZjp368msNybkxg15aCyvvP1RsXUUpOv9Sc8w8YMv+HDVD2xYs5Ljh3+1sakb3IiZixN5f2USbe7pwexpEwFI3vA1Kft+5t1l65n+xVesmPsB//5zziG6LptMfP7mi4yfOY9Ji79ha+IqG0f4/+2deXxV1bXHvz8TAmEeFEjCkBCkTDKDFBWkKjIERBELVYTi7LPjK0pbqzjbWhUr+hStz7HKjKK1aKVi9aHMqFSZBJEEEFFRARmS9f4454abMNwbuMk5uewvn/vhDOvu/bvrnruy9zl7rw3Q89zzuO3517jluVcZMOpqpky83ftM+/fz2M2/ZNT4O7l9yj+54X+mkJIaXJIXKb5XUCRNoFy2ZBE5LXJpntOCtLQ0hl5wEXNfmVPCZu7f53DRT0YBkDd0GG/P/xdWKtXOrOlTGDpseMJ0LVq4kNzcluS08HQN//EIXp7zYgmbl+e8yMWjRgNwwbALeXPeG5gZL895keE/HkHVqlXJzskhN7clixYuTIiuMPpr5YolNG3egibNsqmSlsY5ecOY/3qJRNV0+2FvqqVXB+CUzt34fEtB8bkep/WhRo2aCdESzeoPlpLZLIeMptlUqZJG7wFDWTDvHyVsOvY4vVhX645d+WKrl/1n47rVtO/2Q1JSU6lWvQY5rdqw+O15CdH1ycrlNGySTcOsZqRWSePUfoNZ/tbrJWzSa9Yq3t6ze1dxF3fle2/RpGVrmrVqC0DNuvU4ISUlIbqOhsQtmVM+JE2g3FyQT2bWgXXMM7Ky2Ly5oKTN5gM2qamp1Kpdhy+/3F7C5sWZ0xl6YeK6bgUF+TRpcmBRuKysJuTn5x9s07Rpsa7adeqwfft28vMPfm9BwUELyh0VYfTXti2baZRxYAnmRhmZbNt6+HRjL059ll59ypzVv8xs/3wLJzbOLN4/sVEm2z/fclj7uTP/RrczfgRAix+0Y8nb8/h+9y52fLWd9xe9wxdbCg773rLw9bYt1G+UUbxfr2EGX207WNcb057ihvPPYNqDd3Hxf98CwJaN65HEvT8bxYRRA3n16UcSoumoiLM1GWSLslwf5khqDEwEugNfA1uBCcADQG2gELjDzKaUp454Wbp4IenV02nTtn3QUioFQfrr77On8NEHy3j0+VcqvO4jMW/OdNasXM6fnvTuRXY57UxWf7iM31ySR+16DWjdsRsnpFRs++Ss4aM5a/ho3v3HbOY88SCXT7iPosL9rFm+iD88NYe0aun8+dqRNG/dnrY9Tq9QbQcI99OccvvG5C1vNgt408xyzawr8FsgHbjUzNoB/YGJko45e2pGZhYF+ZuK9zfn55ORkVnSJuOAzf79+/n2mx3Ur9+g+PzsGVM5f1hiHwRkZmaxadOBG/v5+ZvIyso62Oazz4p1fbNjBw0aNCAr6+D3ZmaWfO/REkZ/ndQ4g62bD7SYt24u4KSoFlOE995+k/996F7unfw8aVWrJqz+w9GgYeMSrcAvthbQoGHjg+yWLZjPlMkTufnBp6mSdkDXiKt+xaQZ87jz8WmYGVnNcxOiq+5JjfkyqsX91eebqXfSwboi9Og3hGXzXwO81merzqdSq259qlZL55TT+vLpqg8ToqusRDKch7lFWZ5/2voC+8ysuE1vZivMbL6ZrfH3C4DPgZOOtbJOXbrxybq1fLphPXv37mX2zKn0G5hXwqbfwDym/u0ZAF6ePYPTep+Jv1wlRUVFvDRrOkOHXXSsUkrQrXt31q5dw4b1nq5pU15gUF7JlTEH5Q3huWeeAmDmjOn06fsjJDEobwjTprzAnj172LB+PWvXrqF7jx4J0RVGf7Xt0IWNG9aR/9kG9u3dy+svz6D32QNK2KxauYK7bvwl905+nvonHvNlExet2nemYOMnbNn0Kfv27eWtV2fTs++5JWzWffQBD94yjpsmPU3dBgd0FRYW8s3XXwKwftVKNqz+D116nZkQXTltO7L1s/Vsy9/I/n17ee+1OXQ645wSNls3ri/efv+deTRsmg1A+5592LTuY/Z8v5vC/ftZtfQ9MnNOToiuo+EExfcKivLsercHlhzJQFIPIA1Yd4hzVwJXAjRp2ixmZampqdz554mMvGAQhYVFjLxkNK3btOOPd0ygU+eunDtwMD8Z9VOuu3IMPTu1oW69ejz6xLPF71/wzr/JzGpC85wWZfuUcei6/4FJDB50LoWFhYweM5a27dpx64Sb6NK1G3mDhzBm7GWMHTOKdq1bUq9efZ557gUA2rZrx7DhF9G5Q1tSU1OZ+JeHSEnQDfcw+is1NZXrJ9zDz0cPo7CokCHDLyG3VRseuf8O2pzSmT5nD+SBu25i986djL/Oe/jVOLMJ9z3m+euKiwaw4ZPV7N65k0G92nLj3Q/yw95nHbOulNRUrvndXdx41QiKCgvpd/5ImrdszTOT/sjJ7TrSs29//nrvLXy/ayd3/fpyAE7KyOLmSc9QuH8f4y49D4DqNWvym7sfJiU1MT+7lNRULhl3K/f9/FKKigo5ffBFZOW2Ytaj95LdpgOde5/DG9Oe4j8L3yYltQo1atfm8pvvA6BG7Tqc+5PLuW30YCRxSq++dDz92H11tIR9Zo5KP8VMWMHSz4EcM/vVYc5nAG8Co83s3SOV1bFzV3tt/hFNAqFO9XCumbNj176gJRzEZ9t3BS3hkGzbtSdoCYdky67gxjQeibE9mi8xs26JLLNj5642N87fd0adtITXHw/l2fVeCXQ91AlJtYFXgN/HCpIOhyP5OZ6HB80DqvpdaAAkdZDUB+8hz9NmNr0c63c4HJWAeB/kJOXwIDMzSefjPdW+Afge2AC8C/QGGkga45uPMbPEzRl0OByVirDfoyzXcZT+U+1DPRa9rTzrdTgclYxwx0mXPcjhcARPyOOkC5QOhyNoxAkhXzTHBUqHwxEokZk5YSZpkmI4HA5HeeFalA6HI3DC3qJ0gdLhcATOcT08yOFwOGIS8GDyeHCB0uFwBErQ0xPjwQVKh8MRPCGPlC5QOhyOwAn7OEo3PMjhcAROIrMHSeovaZWktZLGH+J8VUlT/PPvScqOVaYLlA6HI3gSFCklpQAPAQOAtsBISW1LmV0GfGVmLYH7gT/GKtcFSofDETiK818c9ADWmtknZrYXeAE4r5TNecBT/vZ04CzpyH3/SnGP8v3lS79oXCft0wQVdyLwRYLKSiROV9lwuuInkZqaJ6icYpYtXTK3eppOjNO8mqTFUfuTzWxy1H4W8FnU/ibg1FJlFNuY2X5JO4AGHMFHlSJQmlnCVpGStDiIVPKxcLrKhtMVP2HUFI2Z9Q9aQyxc19vhcCQT+UDTqP0m/rFD2khKBeoA249UqAuUDocjmVgEnCwpR1IaMAJ4qZTNS8Bof/tCYJ7FWGWxUnS9E8zk2CaB4HSVDacrfsKoqVzw7zleB8wFUoAnzGylpFuBxWb2EvBX4BlJa4Ev8YLpESm35WodDocjWXBdb4fD4YiBC5QOh8MRAxcoHQ6HIwbHXaCUFNrPHGt2QBA4f5WNsPorjL6qTITySy0PJGUBmFlRmC5mSZ0lDZTUNtYQhYrE+atshNFfYfVVZSQUX2h5I2kwME/SbyE8F7OkAcBUYBDwmqR+/vFA//o7f5VZV+j8FVZfVVaSfniQpObAa3iT45sBq8zsbv/cCWZWFJCu9ngX8tVm9pakS4HxwKlm9m0Qmnxdzl9l09UMz19T8OZBfxy0vyT9AG9Q9RVh8lVlJukDJYCkvsAHeGmXrgOWRi7mADXlAJ3MbFbkByVpDnCJme0IWNtZwArC5a9soIuZzQyLvyR1AfYA7fEGOHcgYH/5mqoCGWHyVWUn8O5UeSGpUWTbzP5lZl8AC4AHgS6RbpKkVpLqVaCuxr6m9cDr/nak1VEbaOTbRaZgVZSuBpKXwcXM3vD99X8E768TJdUxsw14wSgs/uqPN8OjI96Mj68J+PryNT0G5OL9oQuFr5KBpAyUkloDmyXdL+nKyHEz2wcsBSYBLSX9E5iBN9WponQVSJoo6TIz+y7qXBqQDuyWNBJ4AqhWQboGAq8CD0u6I3LczPbjzZ0Nyl8Dgb8Dj0u61cx2+ser+fcA04FdAfirD15AvNLM/mZm66D4+grEX1GarjazZyOaJKVF+arCr61kISkDJfAdXmtoC3ChpKclDfFbJjvN7C1gB17X8mK/9VSRujYDP47SVddPMroE+C3wX8AvzOyb8hbkt0J+B9wB3Ak0lZQeOW9m30f5qw0V5K9Suu4AsiO6fE1FwGLf5loqyF8+XYFJZrZIXvaZ4ockQfnrUJp8PXt9Xy3Au7Yq2ldJQVImxTCzTZIWAl2AgXgZQsYC4yRdD2QCpwPnmtkHIdB1vaSxQC+gLnCOma0ubz2S6uO12IaZ2YuSegDnAH+WlGpmV/l2nYA8oH9F+CteXXgPm3oBPSvIX/KH2eTgBUKAQoDI8BtJHfCSwA6iAvx1JE1RNll411w7KshXyUbStSijhj+MBwwvu/MWvBvtHwLjgJ7AyIoMkjF0fYTXmtsL5FXUhWxmXwKDgZskdcRruU0G7gY6Snret1sOnF5R/opD11Tf9AGgWwX6K/LkcxbQU1JXMzNJJ0QNBzoD2EgF+StOTUOB2UBXFySPEjNLuhfeMkRpwG3Ac8DHwFD/XGugbgh15QD1A9LVHygCxkcdqwn8E2gY4Pd4OF1vALUC1FUDmAD8CS/4RI6PAJYBTUOkaSTe7Z7mQfkrGV5JPTzIH082H3jIzG4LWk+EMOqSdA7eQ4hTzexrST8FrsC7PRHkOMWw6srCW83vLLx7pbvxbqVcaGYfhkzTcKvA3lMyktSBEkDSGCAb+JOZ7QpWzQHCqEvebI57gIfxWkfXBvWjjybEutLxHqKcjfeA7l8WcNc2jJqSgeMhULbG646MCEtAglDrygNmAp3NbGXQeiKEVZfj+CDpAyWApOphCkYRnK6yEVZdjuTnuAiUDofDcSwk3fAgh8PhSDQuUDocDkcMXKB0OByOGLhA6XA4HDFwgTKJkVQoabmkDyVNk1T9GMo6U9LL/vYQSeOPYFtX0rVHUccESb+J93gpmyclXViGurIlBT4W01E5cIEyudltZp3MrD3ePPKro0/Ko8zXgJm9ZEdOTFsXL0uNw5EUuEB5/PBvvByJ2ZJWSXoaL0lIU0n9JC2QtNRvedYEL9WZpI8lLQUuiBQkaYykSf52I0mzJK3wX73wklfk+q3Ze3y7cZIWSXpf0i1RZf1e0mpJbwM/iPUhJF3hl7NC0oxSreSzJS32y8vz7VMk3RNV91WHKdrhOCwuUB4H+PkJB+AthwFwMvCwmbUDdgI3AmebWRe8OcK/llQNL1v2YLwpcY0PU/xfgPlm1hEvlddKvAxJ6/zW7Dh5C1udDPQAOgFdJfWW1BVvSmInvLRz3eP4ODPNrLtf30d4c5sjZPt1DAIe8T/DZcAOM+vul3+FvGU4HI64Scp8lI5i0iUt97f/jbd0QSbwqZm96x/viZfA+B0/E1waXpLX1sB6M1sDIOlZ4EoO5kfApQBmVgjs0MFLH/TzX8v8/Zp4gbMWMCsy20bSS3F8pvaSbsfr3tfEXx7CZ6p5SWrXSPrE/wz9gA5R9y/r+HW7+c+OuHGBMrnZbWadog/4wXBn9CHgdTMbWcquxPuOEQF3mdmjper45VGU9SRearoVfmKRM6POlZ5mZn7dPzOz6IAaWazM4YgL1/V2vAucJqklgKQaklrh5crMlpTr2408zPvfAK7x35siqQ7wLV5rMcJcYGzUvc8sSQ2Bt4ChktIl1cLr5seiFt56SFWAi0udG+4nrM0FWgCr/Lqv8e0ji33ViKMeh6MY16I8zjGzbX7L7HlJVf3DN5rZankLs70iaRde173WIYr4BTBZ0mV4SxBcY2YLJL3jD7951b9P2QZY4Ldov8NbOnWppCl4KwZ+jrcwVyz+ALwHbPP/j9a0EViIt+Lg1Wb2vaTH8e5dLpVX+Ta8jN8OR9y4pBgOh8MRA9f1djgcjhi4QOlwOBwxcIHS4XA4YuACpcPhcMTABUqHw+GIgQuUDofDEQMXKB0OhyMG/w+AbT6Q5taE1gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3254004576659039, array([190, 296, 364, 406, 692, 185]))\n"
     ]
    }
   ],
   "source": [
    "print_confusion = True\n",
    "batch_size = 50\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./natural_language_classifier.ckpt\")\n",
    "    i=0\n",
    "    iteration=0\n",
    "    y_pred = []\n",
    "    while i < X_val.shape[0]:\n",
    "        X_batch, seq_len = get_next_batch_X(X_val, size_texts_val, iteration, batch_size)\n",
    "        #print(X_batch)\n",
    "        temp = sess.run(inference, feed_dict={X: X_batch, seq_lengths: seq_len})\n",
    "        #print(temp)\n",
    "        y_pred.extend(np.argmax(temp, axis=1))\n",
    "        #print(y_pred)\n",
    "        i = i + batch_size\n",
    "        iteration = iteration + 1\n",
    "print(cost(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_RNN_and_train(X_train, y_train, X_train_lengths, X_val, y_val, X_val_lengths, n_steps, n_inputs, \n",
    "                               n_neurons=500, activation=tf.nn.tanh, \n",
    "                               dropout_in=0, class_weights=[1, 1, 1, 1, 1, 1], learning_rate=0.001, \n",
    "                               n_epochs=100, batch_size=200, max_checks_without_progress=3):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs], name=\"X\")\n",
    "    seq_lengths = tf.placeholder(tf.int32, [None]) #vecteur avec les nombres de mots dans les textes\n",
    "    y = tf.placeholder(tf.int64, shape=[None], name=\"y\")\n",
    "    \n",
    "    dropout_in_placeholder = tf.placeholder_with_default(tf.constant(0.0, dtype=tf.float32), ())\n",
    "\n",
    "    cells = [tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(num_units=n, activation=activation), input_keep_prob=1-dropout_in_placeholder, dtype=tf.float32) for n in n_neurons]\n",
    "    stacked_rnn_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    #basic_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, activation=activation)\n",
    "    #basic_cell = tf.contrib.rnn.DropoutWrapper(basic_cell, input_keep_prob=1-dropout_in_placeholder, output_keep_prob=1-dropout_out_placeholder, dtype=tf.float32)\n",
    "    outputs, states = tf.nn.dynamic_rnn(stacked_rnn_cell, X, sequence_length=seq_lengths, dtype=tf.float32)\n",
    "    \n",
    "    #real_outputs = outputs[:,:,seq_lengths] #1 à retirer\n",
    "    idx = tf.range(tf.shape(X)[0])*tf.shape(outputs)[1] + (seq_lengths - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(outputs, [-1, n_neurons[-1]]), idx)\n",
    "    #taille de l'output: [batch_size, n_steps, n_neurons] (par exemple: (200, 418, 500))\n",
    "    #comme les textes sont de tailles variables, on ne peut pas utiliser l'output \n",
    "    \n",
    "    logits = tf.layers.dense(inputs=last_rnn_output, units=n_outputs, name=\"logits\")\n",
    "    inference = tf.nn.softmax(logits, name=\"inference\")\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        #loss = cost(inference, y)\n",
    "\n",
    "        class_weights_tf = tf.constant(class_weights)\n",
    "        weights = tf.gather(class_weights_tf, y)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits, weights=weights)\n",
    "        #xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y) #ancienne version (sans poids)\n",
    "        #loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(tf.cast(logits, tf.float32), y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(\"./summary\", tf.get_default_graph())\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    n_batches_per_epoch = X_train.shape[0] // batch_size\n",
    "    print(\"Nombre de batchs par epoch =\", n_batches_per_epoch)\n",
    "    \n",
    "    best_loss = np.infty\n",
    "    checks_without_progress = 0\n",
    "    early_stopping = False\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, seq_len = get_next_batch_X(X_train, X_train_lengths, iteration, batch_size)\n",
    "                y_batch = y_train[iteration*batch_size:(iteration+1)*batch_size]\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch, seq_lengths: seq_len, dropout_in_placeholder: dropout_in})\n",
    "                #print(sess.run(seq_lengths, feed_dict={X: X_batch, y: y_batch, seq_lengths: seq_len, dropout_in_placeholder: dropout_in, dropout_out_placeholder: dropout_out}).shape)\n",
    "                #sys.exit(0)\n",
    "                if (iteration+1)%10==0:\n",
    "                    print(\"Batch n°\", iteration+1)\n",
    "            #if (iteration+1)%100==0:\n",
    "            # attention l'évaluation du coût est faite en faisant une moyenne de moyennes,\n",
    "            # il faut donc que nb_examples_to_evaluate soit un multiple de batch_size_loss_eval\n",
    "            nb_examples_to_evaluate = np.amin([5000, X_train.shape[0], X_val.shape[0]])\n",
    "            batch_size_loss_eval = batch_size\n",
    "            #if(nb_examples_to_evaluate % batch_size_loss_eval != 0):\n",
    "                #print(\"WARNING: le nombre d'exemples de l'évaluation n'est pas un multiple de batch_size.\")\n",
    "\n",
    "            i=0\n",
    "            k=0\n",
    "            loss_train=0\n",
    "            while i < nb_examples_to_evaluate:\n",
    "                X_batch, seq_len = get_next_batch_X(X_train, X_train_lengths, k, batch_size_loss_eval)\n",
    "                y_batch = get_next_batch_y(y_train, k, batch_size_loss_eval)\n",
    "                temp = loss.eval(feed_dict={X: X_batch, y: y_batch, seq_lengths: seq_len})\n",
    "                #print(temp)\n",
    "                loss_train = loss_train + temp\n",
    "                i = i + batch_size_loss_eval\n",
    "                k = k + 1\n",
    "            #print(loss_train)\n",
    "            #print(k)\n",
    "            loss_train = loss_train / k\n",
    "            print(epoch, \"Loss training on\", nb_examples_to_evaluate, \"examples:\", loss_train)\n",
    "\n",
    "            i=0\n",
    "            k=0\n",
    "            loss_val=0\n",
    "            while i < nb_examples_to_evaluate:\n",
    "                X_batch, seq_len = get_next_batch_X(X_val, X_val_lengths, k, batch_size_loss_eval)\n",
    "                y_batch = get_next_batch_y(y_val, k, batch_size_loss_eval)\n",
    "                temp = loss.eval(feed_dict={X: X_batch, y: y_batch, seq_lengths: seq_len})\n",
    "                loss_val = loss_val + temp\n",
    "                i = i + batch_size_loss_eval\n",
    "                k = k + 1\n",
    "            loss_val = loss_val / k\n",
    "            print(epoch, \"Loss validation on\", nb_examples_to_evaluate, \"examples:\", loss_val)\n",
    "\n",
    "            if loss_val < best_loss:\n",
    "                save_path = saver.save(sess, \"./natural_language_classifier.ckpt\")\n",
    "                best_loss = loss_val\n",
    "                checks_without_progress = 0\n",
    "            else:\n",
    "                checks_without_progress += 1\n",
    "                if checks_without_progress >= MAX_CHECKS_WITHOUT_PROGRESS:\n",
    "                    print(\"Early stopping!\")\n",
    "                    early_stopping = True\n",
    "                    break\n",
    "                #if early_stopping:\n",
    "                    #break\n",
    "\n",
    "    return inference, X, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23292,)\n",
      "(418, 300)\n",
      "Taille vecteur d'un mot = 300\n",
      "Nombre maximal de mots par texte (fixe) = 418\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[0].shape)\n",
    "n_steps = X_train[0].shape[0] #taille des textes (rendue fixe)\n",
    "n_inputs = X_train[0].shape[1] #taille des vecteurs représentant chaque mot\n",
    "print(\"Taille vecteur d'un mot =\", n_inputs)\n",
    "print(\"Nombre maximal de mots par texte (fixe) =\", n_steps)\n",
    "n_neurons = [1500, 1500, 1500]\n",
    "activation = tf.nn.tanh\n",
    "dropout_in = 0.5\n",
    "\n",
    "n_outputs = 6\n",
    "class_weights = [1, 1, 1, 1, 1, 1] #poids de la fonction de coût\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "MAX_CHECKS_WITHOUT_PROGRESS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de batchs par epoch = 232\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "Batch n° 120\n",
      "Batch n° 130\n",
      "Batch n° 140\n",
      "Batch n° 150\n",
      "Batch n° 160\n",
      "Batch n° 170\n",
      "Batch n° 180\n",
      "Batch n° 190\n",
      "Batch n° 200\n",
      "Batch n° 210\n",
      "Batch n° 220\n",
      "Batch n° 230\n",
      "0 Loss training on 5000 examples: 1.5190034461021424\n",
      "0 Loss validation on 5000 examples: 1.7226922464370729\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "Batch n° 120\n",
      "Batch n° 130\n",
      "Batch n° 140\n",
      "Batch n° 150\n",
      "Batch n° 160\n",
      "Batch n° 170\n",
      "Batch n° 180\n",
      "Batch n° 190\n",
      "Batch n° 200\n",
      "Batch n° 210\n",
      "Batch n° 220\n",
      "Batch n° 230\n",
      "1 Loss training on 5000 examples: 1.1615211009979247\n",
      "1 Loss validation on 5000 examples: 1.469264359474182\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "Batch n° 120\n",
      "Batch n° 130\n",
      "Batch n° 140\n",
      "Batch n° 150\n",
      "Batch n° 160\n",
      "Batch n° 170\n",
      "Batch n° 180\n",
      "Batch n° 190\n",
      "Batch n° 200\n",
      "Batch n° 210\n",
      "Batch n° 220\n",
      "Batch n° 230\n",
      "2 Loss training on 5000 examples: 1.0271514129638672\n",
      "2 Loss validation on 5000 examples: 1.4252182173728942\n",
      "Batch n° 10\n",
      "Batch n° 20\n",
      "Batch n° 30\n",
      "Batch n° 40\n",
      "Batch n° 50\n",
      "Batch n° 60\n",
      "Batch n° 70\n",
      "Batch n° 80\n",
      "Batch n° 90\n",
      "Batch n° 100\n",
      "Batch n° 110\n",
      "Batch n° 120\n",
      "Batch n° 130\n",
      "Batch n° 140\n",
      "Batch n° 150\n",
      "Batch n° 160\n",
      "Batch n° 170\n",
      "Batch n° 180\n",
      "Batch n° 190\n",
      "Batch n° 200\n",
      "Batch n° 210\n",
      "Batch n° 220\n",
      "Batch n° 230\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e4b23fe92528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                         \u001b[0mclass_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                         \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                                         batch_size=batch_size, max_checks_without_progress=MAX_CHECKS_WITHOUT_PROGRESS)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-195758cc9604>\u001b[0m in \u001b[0;36mcreate_graph_RNN_and_train\u001b[0;34m(X_train, y_train, X_train_lengths, X_val, y_val, X_val_lengths, n_steps, n_inputs, n_neurons, activation, dropout_in, class_weights, learning_rate, n_epochs, batch_size, max_checks_without_progress)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next_batch_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_loss_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next_batch_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_loss_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0;31m#print(temp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \"\"\"\n\u001b[0;32m--> 656\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5014\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5015\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5016\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inference, X, seq_lengths = create_graph_RNN_and_train(X_train, y_train, size_texts_train, X_val, y_val, size_texts_val,\n",
    "                                                        n_steps, n_inputs, n_neurons=n_neurons, \n",
    "                                                        activation=activation, dropout_in=dropout_in,\n",
    "                                                        class_weights=class_weights,\n",
    "                                                        learning_rate=learning_rate, n_epochs=n_epochs, \n",
    "                                                        batch_size=batch_size, max_checks_without_progress=MAX_CHECKS_WITHOUT_PROGRESS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion = True\n",
    "batch_size = 50\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./natural_language_classifier.ckpt\")\n",
    "    i=0\n",
    "    iteration=0\n",
    "    y_pred = []\n",
    "    while i < X_val.shape[0]:\n",
    "        X_batch, seq_len = get_next_batch_X(X_val, size_texts_val, iteration, batch_size)\n",
    "        #print(X_batch)\n",
    "        temp = sess.run(inference, feed_dict={X: X_batch, seq_lengths: seq_len})\n",
    "        #print(temp)\n",
    "        y_pred.extend(np.argmax(temp, axis=1))\n",
    "        #print(y_pred)\n",
    "        i = i + batch_size\n",
    "        iteration = iteration + 1\n",
    "print(cost(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[0].shape)\n",
    "n_steps = X_train[0].shape[0] #taille des textes (rendue fixe)\n",
    "n_inputs = X_train[0].shape[1] #taille des vecteurs représentant chaque mot\n",
    "print(\"Taille vecteur d'un mot =\", n_inputs)\n",
    "print(\"Nombre maximal de mots par texte (fixe) =\", n_steps)\n",
    "n_neurons = [500, 500, 500]\n",
    "activation = tf.nn.tanh\n",
    "dropout_in = 0.6\n",
    "\n",
    "n_outputs = 6\n",
    "class_weights = [1, 1, 1, 1, 1, 1] #poids de la fonction de coût\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "MAX_CHECKS_WITHOUT_PROGRESS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference, X, seq_lengths = create_graph_RNN_and_train(X_train, y_train, size_texts_train, X_val, y_val, size_texts_val,\n",
    "                                                        n_steps, n_inputs, n_neurons=n_neurons, \n",
    "                                                        activation=activation, dropout_in=dropout_in,\n",
    "                                                        class_weights=class_weights,\n",
    "                                                        learning_rate=learning_rate, n_epochs=n_epochs, \n",
    "                                                        batch_size=batch_size, max_checks_without_progress=MAX_CHECKS_WITHOUT_PROGRESS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion = True\n",
    "batch_size = 50\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./natural_language_classifier.ckpt\")\n",
    "    i=0\n",
    "    iteration=0\n",
    "    y_pred = []\n",
    "    while i < X_val.shape[0]:\n",
    "        X_batch, seq_len = get_next_batch_X(X_val, size_texts_val, iteration, batch_size)\n",
    "        #print(X_batch)\n",
    "        temp = sess.run(inference, feed_dict={X: X_batch, seq_lengths: seq_len})\n",
    "        #print(temp)\n",
    "        y_pred.extend(np.argmax(temp, axis=1))\n",
    "        #print(y_pred)\n",
    "        i = i + batch_size\n",
    "        iteration = iteration + 1\n",
    "print(cost(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[0].shape)\n",
    "n_steps = X_train[0].shape[0] #taille des textes (rendue fixe)\n",
    "n_inputs = X_train[0].shape[1] #taille des vecteurs représentant chaque mot\n",
    "print(\"Taille vecteur d'un mot =\", n_inputs)\n",
    "print(\"Nombre maximal de mots par texte (fixe) =\", n_steps)\n",
    "n_neurons = [400, 300, 100, 50]\n",
    "activation = tf.nn.tanh\n",
    "dropout_in = 0.5\n",
    "\n",
    "n_outputs = 6\n",
    "class_weights = [1, 1, 1, 1, 1, 1] #poids de la fonction de coût\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "MAX_CHECKS_WITHOUT_PROGRESS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference, X, seq_lengths = create_graph_RNN_and_train(X_train, y_train, size_texts_train, X_val, y_val, size_texts_val,\n",
    "                                                        n_steps, n_inputs, n_neurons=n_neurons, \n",
    "                                                        activation=activation, dropout_in=dropout_in,\n",
    "                                                        class_weights=class_weights,\n",
    "                                                        learning_rate=learning_rate, n_epochs=n_epochs, \n",
    "                                                        batch_size=batch_size, max_checks_without_progress=MAX_CHECKS_WITHOUT_PROGRESS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion = True\n",
    "batch_size = 50\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./natural_language_classifier.ckpt\")\n",
    "    i=0\n",
    "    iteration=0\n",
    "    y_pred = []\n",
    "    while i < X_val.shape[0]:\n",
    "        X_batch, seq_len = get_next_batch_X(X_val, size_texts_val, iteration, batch_size)\n",
    "        #print(X_batch)\n",
    "        temp = sess.run(inference, feed_dict={X: X_batch, seq_lengths: seq_len})\n",
    "        #print(temp)\n",
    "        y_pred.extend(np.argmax(temp, axis=1))\n",
    "        #print(y_pred)\n",
    "        i = i + batch_size\n",
    "        iteration = iteration + 1\n",
    "print(cost(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_RNN_and_train(X_train, y_train, X_train_lengths, X_val, y_val, X_val_lengths, n_steps, n_inputs, \n",
    "                               n_neurons=500, activation=tf.nn.tanh, \n",
    "                               dropout_in=0, class_weights=[1, 1, 1, 1, 1, 1], learning_rate=0.001, \n",
    "                               n_epochs=100, batch_size=200, max_checks_without_progress=3):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs], name=\"X\")\n",
    "    seq_lengths = tf.placeholder(tf.int32, [None]) #vecteur avec les nombres de mots dans les textes\n",
    "    y = tf.placeholder(tf.int64, shape=[None], name=\"y\")\n",
    "    \n",
    "    dropout_in_placeholder = tf.placeholder_with_default(tf.constant(0.0, dtype=tf.float32), ())\n",
    "\n",
    "    cells = [tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(num_units=n, activation=activation), input_keep_prob=1-dropout_in_placeholder, dtype=tf.float32) for n in n_neurons]\n",
    "    stacked_rnn_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    #basic_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, activation=activation)\n",
    "    #basic_cell = tf.contrib.rnn.DropoutWrapper(basic_cell, input_keep_prob=1-dropout_in_placeholder, output_keep_prob=1-dropout_out_placeholder, dtype=tf.float32)\n",
    "    outputs, states = tf.nn.dynamic_rnn(stacked_rnn_cell, X, sequence_length=seq_lengths, dtype=tf.float32)\n",
    "    \n",
    "    #real_outputs = outputs[:,:,seq_lengths] #1 à retirer\n",
    "    idx = tf.range(tf.shape(X)[0])*tf.shape(outputs)[1] + (seq_lengths - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(outputs, [-1, n_neurons[-1]]), idx)\n",
    "    #taille de l'output: [batch_size, n_steps, n_neurons] (par exemple: (200, 418, 500))\n",
    "    #comme les textes sont de tailles variables, on ne peut pas utiliser l'output \n",
    "    \n",
    "    logits = tf.layers.dense(inputs=last_rnn_output, units=n_outputs, name=\"logits\")\n",
    "    inference = tf.nn.softmax(logits, name=\"inference\")\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        #loss = cost(inference, y)\n",
    "\n",
    "        class_weights_tf = tf.constant(class_weights)\n",
    "        weights = tf.gather(class_weights_tf, y)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits, weights=weights)\n",
    "        #xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y) #ancienne version (sans poids)\n",
    "        #loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(tf.cast(logits, tf.float32), y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(\"./summary\", tf.get_default_graph())\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    n_batches_per_epoch = X_train.shape[0] // batch_size\n",
    "    print(\"Nombre de batchs par epoch =\", n_batches_per_epoch)\n",
    "    \n",
    "    best_loss = np.infty\n",
    "    checks_without_progress = 0\n",
    "    early_stopping = False\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(n_batches_per_epoch):\n",
    "                X_batch, seq_len = get_next_batch_X(X_train, X_train_lengths, iteration, batch_size)\n",
    "                y_batch = y_train[iteration*batch_size:(iteration+1)*batch_size]\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch, seq_lengths: seq_len, dropout_in_placeholder: dropout_in})\n",
    "                #print(sess.run(seq_lengths, feed_dict={X: X_batch, y: y_batch, seq_lengths: seq_len, dropout_in_placeholder: dropout_in, dropout_out_placeholder: dropout_out}).shape)\n",
    "                #sys.exit(0)\n",
    "                if (iteration+1)%10==0:\n",
    "                    print(\"Batch n°\", iteration+1)\n",
    "            #if (iteration+1)%100==0:\n",
    "            # attention l'évaluation du coût est faite en faisant une moyenne de moyennes,\n",
    "            # il faut donc que nb_examples_to_evaluate soit un multiple de batch_size_loss_eval\n",
    "            nb_examples_to_evaluate = np.amin([5000, X_train.shape[0], X_val.shape[0]])\n",
    "            batch_size_loss_eval = batch_size\n",
    "            #if(nb_examples_to_evaluate % batch_size_loss_eval != 0):\n",
    "                #print(\"WARNING: le nombre d'exemples de l'évaluation n'est pas un multiple de batch_size.\")\n",
    "\n",
    "            i=0\n",
    "            k=0\n",
    "            loss_train=0\n",
    "            while i < nb_examples_to_evaluate:\n",
    "                X_batch, seq_len = get_next_batch_X(X_train, X_train_lengths, k, batch_size_loss_eval)\n",
    "                y_batch = get_next_batch_y(y_train, k, batch_size_loss_eval)\n",
    "                temp = loss.eval(feed_dict={X: X_batch, y: y_batch, seq_lengths: seq_len})\n",
    "                #print(temp)\n",
    "                loss_train = loss_train + temp\n",
    "                i = i + batch_size_loss_eval\n",
    "                k = k + 1\n",
    "            #print(loss_train)\n",
    "            #print(k)\n",
    "            loss_train = loss_train / k\n",
    "            print(epoch, \"Loss training on\", nb_examples_to_evaluate, \"examples:\", loss_train)\n",
    "\n",
    "            i=0\n",
    "            k=0\n",
    "            loss_val=0\n",
    "            while i < nb_examples_to_evaluate:\n",
    "                X_batch, seq_len = get_next_batch_X(X_val, X_val_lengths, k, batch_size_loss_eval)\n",
    "                y_batch = get_next_batch_y(y_val, k, batch_size_loss_eval)\n",
    "                temp = loss.eval(feed_dict={X: X_batch, y: y_batch, seq_lengths: seq_len})\n",
    "                loss_val = loss_val + temp\n",
    "                i = i + batch_size_loss_eval\n",
    "                k = k + 1\n",
    "            loss_val = loss_val / k\n",
    "            print(epoch, \"Loss validation on\", nb_examples_to_evaluate, \"examples:\", loss_val)\n",
    "\n",
    "            if loss_val < best_loss:\n",
    "                save_path = saver.save(sess, \"./natural_language_classifier.ckpt\")\n",
    "                best_loss = loss_val\n",
    "                checks_without_progress = 0\n",
    "            else:\n",
    "                checks_without_progress += 1\n",
    "                if checks_without_progress >= MAX_CHECKS_WITHOUT_PROGRESS:\n",
    "                    print(\"Early stopping!\")\n",
    "                    early_stopping = True\n",
    "                    break\n",
    "                #if early_stopping:\n",
    "                    #break\n",
    "\n",
    "    return inference, X, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[0].shape)\n",
    "n_steps = X_train[0].shape[0] #taille des textes (rendue fixe)\n",
    "n_inputs = X_train[0].shape[1] #taille des vecteurs représentant chaque mot\n",
    "print(\"Taille vecteur d'un mot =\", n_inputs)\n",
    "print(\"Nombre maximal de mots par texte (fixe) =\", n_steps)\n",
    "n_neurons = [500, 250, 100]\n",
    "activation = tf.nn.tanh\n",
    "dropout_in = 0.5\n",
    "\n",
    "n_outputs = 6\n",
    "class_weights = [1, 1, 1, 1, 1, 1] #poids de la fonction de coût\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "MAX_CHECKS_WITHOUT_PROGRESS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inference, X, seq_lengths = create_graph_RNN_and_train(X_train, y_train, size_texts_train, X_val, y_val, size_texts_val,\n",
    "                                                        n_steps, n_inputs, n_neurons=n_neurons, \n",
    "                                                        activation=activation, dropout_in=dropout_in,\n",
    "                                                        class_weights=class_weights,\n",
    "                                                        learning_rate=learning_rate, n_epochs=n_epochs, \n",
    "                                                        batch_size=batch_size, max_checks_without_progress=MAX_CHECKS_WITHOUT_PROGRESS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion = True\n",
    "batch_size = 50\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./natural_language_classifier.ckpt\")\n",
    "    i=0\n",
    "    iteration=0\n",
    "    y_pred = []\n",
    "    while i < X_val.shape[0]:\n",
    "        X_batch, seq_len = get_next_batch_X(X_val, size_texts_val, iteration, batch_size)\n",
    "        #print(X_batch)\n",
    "        temp = sess.run(inference, feed_dict={X: X_batch, seq_lengths: seq_len})\n",
    "        #print(temp)\n",
    "        y_pred.extend(np.argmax(temp, axis=1))\n",
    "        #print(y_pred)\n",
    "        i = i + batch_size\n",
    "        iteration = iteration + 1\n",
    "print(cost(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
